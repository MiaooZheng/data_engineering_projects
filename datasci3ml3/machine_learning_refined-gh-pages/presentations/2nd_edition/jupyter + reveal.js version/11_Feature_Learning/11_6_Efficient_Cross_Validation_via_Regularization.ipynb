{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Chapter 11: Principles of Feature Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 11.6 Efficient Cross-Validation via Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can toggle the code on and off in this presentation via the button below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the previous Section we saw how with boosting based cross-validation we automatically learn the proper level of model complexity for a given dataset by optimizing a general high capacity model one unit at-a-time.   In this Section we introduce what are collectively referred to as *regularization* techniques for efficient cross-validation.  With this set of approaches we once again start with a single high capacity model, and once again adjust its complexity with respect to a training dataset via careful optimization.  However, with regularization we tune all of the units *simultaneously*, controlling how well we *optimize* its associated cost so that a minimum validation instance of the model is achieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## This code cell will not be shown in the HTML version of this notebook\n",
    "# imports from custom library\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import autograd.numpy as np\n",
    "from mlrefined_libraries import nonlinear_superlearn_library as nonlib\n",
    "datapath = '../../mlrefined_datasets/nonlinear_superlearn_datasets/'\n",
    "\n",
    "# plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# this is needed to compensate for %matplotlib notebook's tendancy to blow up images when plotted inline\n",
    "%matplotlib notebook\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 11.6.1 The big picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Imagine for a moment that we have a simple nonlinear regression dataset, like the one shown in the top-left panel of the [Figure 11.37](#figure-11-37), and we use a high capacity model (relative to the nature of the data) made up of a sum of *universal approximators* of a single kind to fit it as\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{model}\\left(\\mathbf{x},\\Theta\\right) = w_0 + f_1\\left(\\mathbf{x}\\right){w}_{1} +  f_2\\left(\\mathbf{x}\\right){w}_{2} + \\cdots + f_M\\left(\\mathbf{x}\\right)w_M.\n",
    "\\label{equation:regularization-original-construct}\n",
    "\\end{equation}\n",
    "\n",
    " Suppose then that we partition this data into training and validation portions, and then train our high capacity model by *completely* optimizing the Least Squares cost over the training portion of the data.  In other words, we determine a set of parameters for our high capacity model that lie very close to a global minimum of its associated cost function.  In the top-right panel of the Figure we draw a hypothetical two dimensional illustration of the cost function associated with our high capacity model over the training data, denoting the global minimum by a blue dot and its evaluation on the function by a blue 'x'.  \n",
    "\n",
    "Since our model has high capacity, the resulting fit provided by the parameters lying at the global minimum of our cost will produce a tuned model that is overly complex and *severely* overfits the training portion of our dataset.  In the bottom-left panel of the [Figure 11.37](#figure-11-37) we show the tuned model fit (in blue) provided by such a set of parameters, which wildly overfits the training data.  In the top-right panel we also show a set of parameters lying relatively near the global minimum as a yellow dot, and whose evaluation of the function is shown as a yellow 'x'.  This set of parameters lying in the general neighborhood of the global minimum is where the cost function is minimized over the *validation* portion of our data.  Because of this the corresponding fit (shown in the bottom-right panel in yellow) provides a much better representation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a id='figure-11-37'></a>\n",
    "<figure>\n",
    "<p>\n",
    "  <img src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_37.png' width=\"70%\"  alt=\"\"/>\n",
    "</p>\n",
    "<figcaption> <strong>Figure: 11.37 </strong> <em> \n",
    "(top row) (top-left panel) A generic nonlinear regression dataset. (top-right panel) A figurative illustration of the cost function associated with a high capacity model over the training portion of this data.  The global minimum is marked here with a blue dot (along with its evaluation by a blue 'x') and a point nearby is marked in yellow (and whose evaluation is shown as a yellow 'x').  (bottom-left panel) The original data and fit (in blue) provided by the model using parameters from the global minimum of the cost function severely overfits the training portion of the data.  (bottom-right panel)  A fit provided by the parameters corresponding to the yellow dot shown in the top-right panel minimize the cost function over the validation portion of the data, and here provide a much better fit (in yellow) to the data.  See text for further details.\n",
    "</em>\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Overfitting occurs when:\n",
    "- the *capacity* of a machine learning model is too high\n",
    "- *and*, its corresponding cost function (over the training data) is *optimized* too well.\n",
    "\n",
    "\n",
    "#### Regularization - a potential solution:\n",
    "- we set the model parameters purposefully away from the global minimum of its associated cost function, so as to find where the validation error (<u>not</u> training error) is at its lowest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two most popular approaches to regularization:\n",
    "- Early stopping\n",
    "- Adding a regularizer to the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<p>\n",
    "  <img src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_38.png' width=\"70%\"  alt=\"\"/>\n",
    "</p>\n",
    "\n",
    "**(left panel)** A figurative illustration of early stopping regularization: we stop the optimization prematurely at the yellow point (where validation error is minimal).<br>\n",
    "**(right panel)** A figurative illustration of regularizer based regularization: by adding a regularizer function to the cost associated of a high capacity model we change its shape, in particular dragging its global minimum where overfitting behavior occurs away from its original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Early stopping based regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "With *early stopping regularization* we properly tune a high capacity model by making a run of local optimization (tuning all parameters of the model), and by using the set of weights from this run where the model achieves minimum validation error.  This idea is illustrated in the left panel of [Figure 11.38](#figure-11-38) where we employ the same prototypical cost function (associated with a high capacity model) first shown in the top-right panel of [Figure 11.38](#figure-11-38).\n",
    "\n",
    "Here again we mark its global minimum and set of validation error minimizing weights in blue and yellow, respectively (as detailed originally in [Figure 11.37](#figure-11-37). During a run of local optimization we frequently compute training and validation errors (e.g., at each step of the optimization procedure). Thus, depending on the optimization procedure used (as detailed further below) a set of weights providing minimum validation error for a high capacity model can be determined with fine resolution.\n",
    "\n",
    "This regularization approach is especially popular when employing high capacity deep neural network models as detailed in Section 13.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Whether or not one literally stops the optimization run when minimum validation error has been reached (which can be challenging in practice given the somewhat unpredictable behavior of validation error as first noted in Section 11.4) or one runs the optimization to completion (picking the best set of weights afterwards), in either case we refer to this method as early stopping regularization.  Note that the method itself is analogous to the early stopping procedure outlined for boosting based cross-validation in Section 11.5 in that we sequentially increase the complexity of a model until minimum validation is reached.   However, here (unlike boosting) we do this by controlling how well we optimize a model's parameters *simultaneously*, as opposed to one unit at-a-time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "Supposing that we begin our optimization with a small initial value (which we typically do; see for example, Section 3.6) the corresponding training and validation error curves will in general\\footnote{Note that both can oscillate in practice depending on the optimization method used.} look like those shown in top panel of [Figure 11.38](#figure-11-38). At the start of the run the complexity of our model (evaluated at, for instance, the initial weights) is quite small, providing a large training and validation error.  As minimization proceeds, and we continue optimizing one step at-a-time, error in both training and validation portions of the data decreases while the complexity of the tuned model increases.  This trend continues up until a point when the model complexity becomes too great and overfitting begins, and validation error increases.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<figure>\n",
    "<p>\n",
    "  <img align=\"right\" src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_39.png' width=\"60%\"  alt=\"\"/>\n",
    "</p>\n",
    "    \n",
    "**(top panel)** A prototypical pair of training/validation error curves associated with a generic run of the early stopping regularization.<br><br>\n",
    "**(bottom panels)** We set our capacity dial all the way to the right and our optimization dial all the way to the left, slowly moving it from left to right in search of a model with minimum validation error. Here each notch on the optimization dial abstractly denotes a step of local optimization.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In terms of the capacity/optimization dial scheme detailed in the context of real data in Section 11.3.2, we can think of (early stopping based) regularization as beginning with our capacity dial set all the way to the right (since we employ a high capacity model) and our optimization dial all the way to the left (at the initialization of our optimization).  With this configuration - summarized visually in the bottom panel of [Figure 11.39](#figure-11-39) - we allow our optimization dial to (roughly speaking) directly govern the amount of complexity our tuned models can take (here each notch on the optimization dial denotes a single step of local optimization).  In other words, with this configuration our optimization dial becomes (roughly speaking) the ideal complexity dial described at the start of the Chapter in Section 11.1.  With early stopping we turn our optimization dial from left to right, starting at our initialization making a run of local optimization one step at-a-time, seeking out a set of parameters that provide minimum validation error for our (high capacity) model.  This is illustrated in the bottom panels of Figure 11.39, where we see our capacity dial set all the way to the right and our generic validation error curve wrapped around our optimization dial (as it now, roughly speaking, controls the complexity of each tuned model).\n",
    "\n",
    "With an initial set of parameters $\\Theta_0$, taking $M$ steps of a local optimization produces a sequence of $M+1$ parameter settings $\\left\\{\\Theta_m\\right\\}_{m=0}^M$ for our model, or similarly (ignoring the initialization for the sake of illustration) a set of $M$ models of generally *increasing* complexity with respect to the training data $\\left\\{\\text{model}\\left(\\textbf{x},\\Theta_m\\right)\\right\\}_{m=1}^M$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a id='figure-11-40'></a>\n",
    "<figure>\n",
    "<p>\n",
    "  <img src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_40_bottom.png' width=\"90%\"  alt=\"\"/>\n",
    "</p>\n",
    "<figcaption> <strong>Figure: 11.40 </strong> <em> \n",
    "Two subtleties associated with early stopping based regularization.  (top row) (left panel) A prototypical cost function associated with a high capacity model, with two optimization paths (shown in red and green, respectively) resulting from two local optimization runs beginning at different starting points.  (middle panel) The validation error histories corresponding to each optimization run.  (right panel) While each run produces a different set of optimal weights, and a different fit to the data (here shown in green and red respectively, corresponding to each run), these fits are generally equally representative.  (bottom row) (left panel) Taking optimization steps with a small steplength makes the early stopping procedure a fine-resolution search for optimal model complexity.  With such small steps we smoothly turn the optimization dial from left to right in search of a validation error minimizing tuned model.  (right panel)  Using steps with a large steplength makes early stopping a coarse resolution search for optimal model complexity.  With each step taken we aggressively turn the dial from left to right, performing a coarser resolution model search that potentially skips over the optimal model.\n",
    "</em>\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "There are a number of important engineering details associated with\n",
    "implementing an effective early stopping regularization procedure, including the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Is the model found by early stopping unique?\n",
    "\n",
    "Different initializations can produce different trajectories towards potentially different minima of the cost function, and produce corresponding validation error minimizing models that differ in shape.\n",
    "\n",
    "<br>\n",
    "<p>\n",
    "  <img src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_40_top.png' width=\"90%\"  alt=\"\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How high should capacity be set?\n",
    "\n",
    "While there is no clear-cut answer to this question, the capacity must simply be set at least 'high' enough that the model overfits if optimized completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "-  **Local optimization must be carefully performed.** One must be careful with the sort of local optimization scheme used with early stopping cross-validation.  As illustrated in the bottom row of [Figure 11.40](#figure-11-40), ideally we want to turn our optimization dial smoothly from left to right, searching over a set of model complexities with a fine resolution (depicted visually in the bottom-left panel of the Figure).  This means - for example - that with early stopping we often avoid local optimization schemes that take very large steps (e.g., Newton's method - as detailed in Chapter 2) as this can result in a coarse low resolution search over model complexity that can easily skip over minimum validation models, as depicted in the bottom-right panel of the Figure.  Local optimizers that take smaller, high quality steps - like the advanced first order methods detailed in Chapter 3 - are often preferred when employing early stopping.  Moreover when employing a mini-batch/stochastic first order methods (see Section 3.11) validation error should be measured *several times per epoch* to avoid taking too large of steps without measuring validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When is validation error really at its lowest?\n",
    "\n",
    "- While generally speaking validation error decreases at the start of an optimization run and eventually increases (making somewhat of a 'U' shape) it can certainly fluctuate up and down during optimization.\n",
    "\n",
    "- Often in practice a reasonable engineering choice is made as to when to stop based on how long it has been since the validation error has *not* decreased. \n",
    "\n",
    "- Moreover, one need not truly halt a local optimization procedure to employ the thrust of early stopping, and can simply run the optimizer to completion and select the best set of weights from the run (that minimize validation error) after completion.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularizer based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A <b><u>regularizer</u></b> is a simple function that can be added to a machine learning cost for a variety of purposes:\n",
    "- to prevent unstable learning\n",
    "- as a natural part of relaxing the support vector machine and multi-class learning scenarios\n",
    "- for feature selection\n",
    "- and, for regularization (which we discuss here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Denoting the original cost function by $g$, and the associated regularizer by $h$, the regularized cost is given as the linear combination of $g$ and $h$ as \n",
    "\n",
    "\\begin{equation}\n",
    "    g\\left(\\Theta \\right) + \\lambda\\, h\\left(\\Theta\\right)\n",
    "    \\label{equation:cross-validation-general-regularized-cost}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\lambda$ is referred to as the *regularization parameter*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The regularization parameter \n",
    "- is always non-negative $\\lambda \\geq 0$ \n",
    "- controls the mixture of the cost and regularizer \n",
    "- when set very small, the regularized cost is essentially just $g$\n",
    "- when set very large the regularizer $h$ dominates in the linear combination and drowns out $g$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "  <img align=\"right\" src= '../../mlrefined_images/nonlinear_superlearn_images/11_41.png' width=\"55%\"  alt=\"\"/>\n",
    "<b>(top panel)</b> A prototypical pair of training/validation error curves associated with a generic run of regularizer based cross-validation. <br><br>\n",
    "\n",
    "<b>(bottom panels)</b> Initially, the capacity dial is set all the way to the right and the optimization dial all the way to the left. We then slowly move our optimization dial from left to right by decreasing the value of $\\lambda$, gradually increasing the complexity of our tuned model, in search of a tuned model with minimum validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Supposing that we begin with a large value of $\\lambda$ and try progressively smaller values (completely optimizing each regularized cost) - the corresponding training and validation error curves will in general look something like those shown in the top panel of [Figure 11.41](#figure-11-41) (remember in practice that *validation error* can oscillate, and need not take just one dip down). At the start of this procedure, using a large value of $\\lambda$, the complexity of our model is quite small as the regularizer completely dominated in the regularized cost, and thus the associated minimum recovered belongs to the regularizer and not the cost function itself.  Since the set of weights is virtually unrelated to the data we are training over the corresponding model will tend to have large training and validation errors.  As $\\lambda$ is decreased the parameters provided by complete minimization of the regularized cost will be closer to the global minima of the original cost itself, and so error on both training and validation portions of the data decreases while (generally speaking) the complexity of the tuned model increases.  This trend continues up until a point when the regularization parameter is small enough that the recovered parameters lie too close to that of the original cost, so that the corresponding model complexity becomes too great.  Here overfitting begins and validation error increases.\n",
    "\n",
    "In terms of the capacity/optimization dial scheme detailed in the context of real data in Section 11.3.2, we can think of (regularizer-based) regularization as beginning with our capacity dial set to the *right* (since we employ a high capacity model) and our optimization dial all the way to the *left* (employing a large value for $\\lambda$ in our regularized cost).  Here each notch on the optimization dial represents the complete minimization of the regularized cost function) for a given value of $\\lambda$ - thus when the dial is turned all the way to the right (where $\\lambda = 0$) we completely minimize the original cost.  With this configuration (summarized visually in the bottom panel of Figure 11.41) we allow our optimization dial to (roughly speaking) directly govern the amount of complexity our tuned models can take (here each setting of the capacity dial defines a model and each setting of the optimization dial a set of parameters of that model).  As we turn our optimization dial from left to right we *decrease* the value of $\\lambda$ and *completely minimize* the corresponding regularized cost, seeking out a set of parameters that provide minimum validation error for our (high capacity) model.  This is illustrated in the bottom panels of [Figure 11.41](#figure-11-41), where we see our capacity dial set all the way to the right and our generic validation error curve wrapped around our optimization dial (as it now, roughly speaking, controls the complexity of each tuned model).\n",
    "\n",
    "With an set of $M$ values $\\left\\{\\lambda_m\\right\\}_{m=1}^M$ for our regularization parameter $\\lambda$, sorted from *largest to smallest* ($\\lambda_1$ being the largest value chosen and $\\lambda_M$ the smallest) this scheme produces a sequence of $M$ parameter settings $\\left\\{\\Theta_m\\right\\}_{m=1}^M$ and corresponding models $\\left\\{\\text{model}\\left(\\textbf{x},\\Theta_m\\right)\\right\\}_{m=1}^M$ of generally *increasing* complexity. Thus, formally speaking, we can see regularizer based regularization stopping falls into the general category of cross-validation techniques outlined in Section 11.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p>\n",
    "  <img src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_42_bottom.png' width=\"90%\"  alt=\"\"/>\n",
    "</p>\n",
    "<b>(left panel)</b> Testing out a large range and number of values for the regularization parameter $\\lambda$ results in a fine resolution search for validation error minimizing weights.<br><br>\n",
    "<b>(right panel)</b> A smaller number or a poorly chosen range of values can result in a coarse search that can skip over ideal weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementation notes \n",
    "\n",
    "- Bias weights are often not included in the regularizer.\n",
    "\n",
    "- How many values we can try to tune the regularization parameter is often limited by computation and time restrictions, since for every value of $\\lambda$ tried a complete minimization of a corresponding regularized cost function must be performed.\n",
    "\n",
    "- While the squared $\\ell_2$ norm is a very popular regularizer, one can use - in principle - any simple function as a regularizer. A few examples follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Popluar regularizers in machine learning\n",
    "\n",
    "- **The squared $\\ell_2$ norm:** defined as $\\Vert \\mathbf{w} \\Vert_2^{2} = \\sum_{n=1}^{N} w_{n}^2$, this regularizer incentivizes weights to be <u>*small*</u> (in Euclidean sense).  \n",
    "\n",
    "- **The $\\ell_1$ norm:** defined as $\\Vert \\mathbf{w} \\Vert_1 = \\sum_{n=1}^{N} \\vert w_{n}\\vert$, this regularizer tends to produce <u>*sparse*</u> weights.\n",
    "\n",
    "- **The total variation norm:** defined as$\\Vert \\mathbf{w} \\Vert_{\\text{TV}} = \\sum_{n=1}^{N-1} \\vert w_{n+1} - w_n\\vert$, this regularizer tends to produce <u>*smoothly varying*</u> weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p>\n",
    "  <img src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_42_top.png' width=\"90%\"  alt=\"\"/>\n",
    "</p>\n",
    "\n",
    "A visual depiction of where the $\\ell_2$ **(left panel)**, $\\ell_1$ **(middle panel)**, and total variation **(right panel)** functions pull global minima when used as a regularizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Below we illustrate the use of regularizer based regularization on a simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example.</span>   Tuning a regularization parameter for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In this example we use an $\\ell_2$-regularized degree-10 polynomial model.\n",
    " \n",
    "- The training set/error is shown in light blue while the validation set/error is shown in yellow.\n",
    " \n",
    "- We try out $100$ values of $\\lambda$ (i.e., the regularization parameter) between $0$ and $1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"1000\" height=\"400\" controls loop>\n",
       "  <source src=\"videos/animation_10.mp4\" type=\"video/mp4\">\n",
       "  </video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code cell will not be shown in the HTML version of this notebook\n",
    "import copy\n",
    "\n",
    "# load in dataset\n",
    "csvname = datapath + 'noisy_sin_sample.csv'\n",
    "data = np.loadtxt(csvname,delimiter = ',')\n",
    "x = data[:-1,:]\n",
    "y = data[-1:,:] \n",
    "\n",
    "# start process\n",
    "num_units = 100\n",
    "degree = 10\n",
    "train_portion = 0.66\n",
    "lambdas = np.linspace(0,1,num_units)\n",
    "\n",
    "runs1 = []\n",
    "w = 0\n",
    "for j in range(num_units):\n",
    "    lam = lambdas[j]\n",
    "    \n",
    "    # initialize with input/output data\n",
    "    mylib1 = nonlib.intro_general_library.superlearn_setup.Setup(x,y)\n",
    "    \n",
    "    # define feature transforms\n",
    "    mylib1.choose_features(name = 'polys',degree = degree)\n",
    "\n",
    "    # standard normalize input\n",
    "    mylib1.choose_normalizer(name = 'standard')\n",
    "\n",
    "    # split into training and validation sets\n",
    "    if j == 0:\n",
    "        # make training testing split\n",
    "        mylib1.make_train_valid_split(train_portion = train_portion)\n",
    "        train_inds = mylib1.train_inds\n",
    "        valid_inds = mylib1.valid_inds\n",
    "\n",
    "    else: # use split from first run for all further runs\n",
    "        mylib1.x_train = mylib1.x[:,train_inds]\n",
    "        mylib1.y_train = mylib1.y[:,train_inds]\n",
    "        mylib1.x_valid = mylib1.x[:,valid_inds]\n",
    "        mylib1.y_valid = mylib1.y[:,valid_inds]\n",
    "        mylib1.train_inds = train_inds\n",
    "        mylib1.valid_inds = valid_inds\n",
    "        mylib1.train_portion = train_portion\n",
    "\n",
    "    # choose cost\n",
    "    mylib1.choose_cost(name = 'least_squares',lam=lam)\n",
    "\n",
    "    if j == 0:\n",
    "        # fit an optimization\n",
    "        mylib1.fit(optimizer = 'newtons_method',max_its = 1,verbose = False)\n",
    "    else:\n",
    "        mylib1.fit(optimizer = 'newtons_method',max_its = 1,verbose = False,w=w,epsilon=10**(-12))\n",
    "\n",
    "    # add model to list\n",
    "    runs1.append(copy.deepcopy(mylib1))\n",
    "    w = mylib1.w_init\n",
    "    \n",
    "# animate the business\n",
    "frames = 100\n",
    "csvname = datapath + 'noisy_sin_sample.csv'\n",
    "demo1 = nonlib.regularization_regression_animators.Visualizer(csvname)\n",
    "savepath = 'videos/animation_10.mp4'\n",
    "demo1.animate_trainval_regularization(savepath,runs1,frames,num_units,show_history = True,fps = 10)\n",
    "\n",
    "# load in video and display\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"1000\" height=\"400\" controls loop>\n",
    "  <source src=\"videos/animation_10.mp4\" type=\"video/mp4\">\n",
    "  </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 11.6.4 Similarity to regularization for feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Akin to the boosting procedure detailed in the previous Section, here the careful reader will notice how similar the regularizer based regularization framework described here is to the concept of regularization detailed for feature selection in [Section 9.7](https://jermwatt.github.io/machine_learning_refined/notes/9_Feature_engineer_select/9_7_Regularization.html).  The two approaches are very similar in theme, except here we do not select from a set of given input features but *create them ourselves based on a universal approximator*. Additionally, instead of our main concern with regularization being *human interpret-ability* of a machine learning model here we use regularization as a tool for cross-validation."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "199px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "817.559px",
    "left": "0px",
    "right": "1588.03px",
    "top": "112.422px",
    "width": "211.973px"
   },
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
