{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Chapter 11: Principles of Feature Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 11.9 Bagging Cross-Validated Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can toggle the code on and off in this presentation via the button below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The random nature of splitting data into training and validation poses a potential flaw to our cross-validation process: <b><u>bad training-validation splits</u></b>.\n",
    "\n",
    "\n",
    "- Such bad splits are not desirable representatives of the underlying phenomenon that generated them, which can result in poorly representative cross-validated models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- A practical solution to this fundamental problem is to simply perform several different training-validation splits, determine an appropriate cross-validated model on each split, and then *average* the resulting cross-validated models. This is called <b><u>bagging</u></b>.\n",
    "\n",
    "\n",
    "- By averaging a set of cross-validated models we can *very often* both 'average out' the potentially undesirable characteristics of each model while synergizing their positive attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## This code cell will not be shown in the HTML version of this notebook\n",
    "# imports from custom library\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import autograd.numpy as np\n",
    "from mlrefined_libraries import nonlinear_superlearn_library as nonlib\n",
    "datapath = '../../mlrefined_datasets/nonlinear_superlearn_datasets/'\n",
    "\n",
    "# plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# this is needed to compensate for %matplotlib notebook's tendancy to blow up images when plotted inline\n",
    "%matplotlib notebook\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Generally the best way to bag (or average) cross-validated regression models is by taking their <b><u>median</u></b> (as opposed to their mean)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example.</span>   Bagging cross-validated regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the set of small panels in the left side of [Figure 11.47](#figure-11-47) we show $10$ different training-validation splits of a prototypical nonlinear regression dataset, where\n",
    "$\\frac{4}{5}$ of the data in each instance has been used for training (colored light blue) and $\\frac{1}{5}$ is used for validation (colored yellow).  Plotted with each split of the original data is the corresponding cross-validated spanning set model found by naively cross-validating (see [Section 11.4](https://jermwatt.github.io/machine_learning_refined/notes/11_Feature_learning/11_4_Cross_validation.html)) the range of complete polynomial models of degree $1$ to $20$.  As we can see, while *many* of these cross-validated models perform quite well, several of them (due to the particular training-validation split on which they are based) severely *underfit* or *overfit* the original dataset.  In each instance the poor performance is completely due to the particular underlying (random) training-validation split, which leads cross-validation to a validation error minimizing tuned model that still does not represent the true underlying phenomenon very well.  By taking an *average* (here the *median*) of the $10$ cross-validated models shown in these small panels we can average-out the poor performance of this handful of bad models, leading to a final bagged model that fits the data quite well - as shown in the large right panel of [Figure 11.47](#figure-11-47).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id='figure-11-47'></a>\n",
    "<figure>\n",
    "<p>\n",
    "  <img align=\"right\" src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_47.png' width=\"70%\"  alt=\"\"/>\n",
    "</p>\n",
    "<figcaption> <em> (small panels) Ten different random training-validation splits of a nonlinear regression dataset (blue: training, yellow: validation), with the best cross-validated model drawn in each panel.<br><br> (large panel) The bagged (median) model of the $10$ models whose fits are shown on the left. \n",
    "</em>\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "More generally, if we created $E$ cross-validated regression models $\\left\\{\\text{model}_{e}\\left(\\mathbf{x},\\Theta_e^{\\star}\\right)\\right\\}_{e=1}^E$, each trained on a different training-validation split of the data, then our median model $\\text{model}\\left(\\mathbf{x},\\Theta\\right)$ as \n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{model}\\left(\\mathbf{x},\\Theta^{\\star} \\right) = \\text{median}\\left\\{ \\text{model}_{e}\\left(\\mathbf{x},\\Theta_e^{\\star}\\right)\\right\\}_{e=1}^E.\n",
    "\\end{equation}\n",
    "\n",
    "Note here the parameter set $\\Theta^{\\star}$ of the median model contains all of tuned parameters $\\left\\{\\Theta_e^{\\star}\\right\\}_{e=1}^E$ from the models in cross-validated set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why the median, not the mean?\n",
    "\n",
    "\n",
    "Because generally speaking, the mean is more sensitive to *outliers* the median.\n",
    "<br><br><br>\n",
    "<figure>\n",
    "    \n",
    "<p>\n",
    "  <img src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_48_1.png' width=\"80%\"  alt=\"\"/>\n",
    "</p>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bagging polynomials, neural networks, and trees together  \n",
    "\n",
    "\n",
    "With bagging we can also effectively combine cross-validated models built from different universal approximators.\n",
    "\n",
    "<br>\n",
    "<figure>\n",
    "    \n",
    "<p>\n",
    "  <img src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_48_2.png' width=\"80%\"  alt=\"\"/>\n",
    "</p>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a id='figure-11-48'></a>\n",
    "<figure>\n",
    "<p>\n",
    "  <img src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_48.png' width=\"90%\"  alt=\"\"/>\n",
    "</p>\n",
    "<figcaption> <strong>Figure: 11.48 </strong> <em> \n",
    "(top row) The $10$ individual cross-validated models first shown in the left column of Figure 11.47 shown together.  The median and mean of these models are shown in the middle and right panel, respectively.  With regression, bagging via the median tends to produce more trustworthy results as it is less sensitive to outliers. (bottom row) Cross-validated fixed-shape polynomial (left panel), neural network (second panel from the left), and tree-based (second panel from the right) models.  The median of these three models is shown in the right panel.  See text for further details.\n",
    "</em>\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "When we bag we are simply averaging various cross-validated models with the desire to both avoid bad aspects of poorly performing models, and jointly leverage strong elements of the well performing ones.  Nothing in\n",
    "this notion prevents us from bagging together cross-validated models built using different universal approximators, and indeed this is the most organized way of combining different types of universal approximators in practice.\n",
    "\n",
    "In the bottom row of [Figure 11.48](#figure-11-48) we show the result of a cross-validated polynomial model (left panel) built by naively cross-validating full polynomials of degree $1$ through $10$, a naively cross-validated (see [Section 11.4](https://jermwatt.github.io/machine_learning_refined/notes/11_Feature_learning/11_4_Cross_validation.html)) neural network model (in the second to the left panel) built by comparing models consisting of $1$ through $10$ units, and a cross-validated stump model (second to the right panel) built via boosting (see [Section 11.5](https://jermwatt.github.io/machine_learning_refined/notes/11_Feature_learning/11_5_Boosting.html)).  Each cross-validated model uses a different training-validation split of the original dataset, and the bagged (median) of these models is shown in the right panel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Because the predicted output of a classification model is a *discrete* label, the average used to bag such cross-validated models is the <b><u>mode</u></b> (i.e., the\n",
    "most popularly predicted label)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example.</span>   Bagging cross-validated two-class classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the set of small panels in the left column of [Figure 11.49](#figure-11-49) we show $5$ different training-validation splits of the prototypical two-class classification dataset, where $\\frac{2}{3}$ of the data in each instance is used for training and $\\frac{1}{3}$ is used for validation (the edges of these points are colored yellow).  Plotted with each split of the original data is the nonlinear decision boundary corresponding to each  cross-validated model found by naively cross-validating the range of complete polynomial models of degree $1$ to $8$.  *Many* of these cross-validated models perform quite well, but several of them (due to the particular training-validation split on which they are based) severely *overfit* the original dataset.  By bagging these models using the most popular prediction to assign labels (i.e., the *mode* of these cross-validated model predictions) we produce an appropriate decision boundary for the data shown in the right panel of the Figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id='figure-11-49'></a>\n",
    "<figure>\n",
    "<p>\n",
    "  <img align=\"right\" src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_49.png' width=\"50%\"  alt=\"\"/>\n",
    "</p>\n",
    "<figcaption> <em> \n",
    "(small panels) Five models cross-validated on random training-validation splits of the data, with the validation data in each instance highlighted with a yellow outline.<br><br> (large panel) The bagged (modal) model of the $5$ models shown on the left.   \n",
    "</em>\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the top-middle panel of [Figure 11.50](#figure-11-50)  we illustrate the decision boundary from each of $5$ naively cross-validated models each built using $B = 20$ single layer $\\text{tanh}$ units trained on different training / validation splits\n",
    "of the dataset shown in the top-left panel of the Figure.  In each instance $\\frac{1}{3}$ of the dataset is randomly chosen as validation (and is highlighted in yellow), and the appropriate tuning of each model's parameters is achieved via $\\ell_2$ regularization based cross-validation (see [Section 11.6](https://jermwatt.github.io/machine_learning_refined/notes/11_Feature_learning/11_6_Regularization.html)) using a dense range of values for $\\lambda \\in [0,0.1]$.  In the top-middle panel we plot the diverse set of decision boundaries associated to each cross-validated model on top of the original dataset, each colored differently so they can be distinguished visually. While some of these decision boundaries separate the two classes quite well, others do a poorer job. In the top-right panel we show the decision boundary of the bag, created by taking the mode of the predictions from these cross-validated models, which performs quite well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a id='figure-11-50'></a>\n",
    "<figure>\n",
    "<p>\n",
    "  <img src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_50.png' width=\"90%\"  alt=\"\"/>\n",
    "</p>\n",
    "<figcaption> <strong>Figure: 11.50 </strong> <em> \n",
    "(top row) (middle panel) The decision boundaries, each shown in a different color, resulting from $5$ models cross-validated on different training-validation splits of the dataset shown in the left panel.  (right panel) The decision boundary resulting from the mode, the 'modal model,' of the $5$ cross-validated models whose decision boundaries are shown in the middle panel.  (bottom row) The decision boundaries provided by a cross-validated fixed-shape model (left panel), neural network model (second from the left panel), and tree-based model (second panel from the right).  In each instance the validation portion of the data is highlighted in yellow.  (right panel)  The decision boundary provided by the mode of these three models.  See text for further details.\n",
    "</em>\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As with regression, with classification we can also combine cross-validated models built from different universal approximators.  We illustrate this in the bottom row of [Figure 11.50](#figure-11-50) using the same dataset.   In particular we show the result of a naively cross-validated polynomial model (left panel) built by comparing full polynomials of degree $1$ through $10$, a naively cross-validated neural network model (in the second to the left panel) built by comparing models consisting of $1$ through $10$ units, and a cross-validated stump model (second to the right panel) built via boosting over a range of $20$ units.  Each cross-validated model uses a different training-validation split of the original dataset (the validation data portion highlighted in yellow in each panel), and the bag (mode) of these models shown in the right panel performs quite well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bagging different types of universal approximators\n",
    "\n",
    "\n",
    "As with regression, with classification we can also combine cross-validated models built from different universal approximators.\n",
    "\n",
    "<br>\n",
    "<figure>\n",
    "    \n",
    "<p>\n",
    "  <img src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_50_2.png' width=\"80%\"  alt=\"\"/>\n",
    "</p>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example.</span>   Bagging multi-class models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In this example we illustrate the bagging of various cross-validated multi-class models on two different datasets, shown in the left column of [Figure 11.51](#figure-11-51).  In each case we naively cross-validate  full polynomial models of degree $1$ through $5$, with $5$ cross-validated models learned in total.  In the middle column of the Figure we show the decision boundaries provided by each cross-validated model in distinct colors, while the decision boundary of the final modal model is shown in the right column for each dataset, which perform very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<a id='figure-11-51'></a>\n",
    "<figure>\n",
    "<p>\n",
    "  <img src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_51.png' width=\"70%\"  alt=\"\"/>\n",
    "</p>\n",
    "<figcaption> <em> \n",
    "</em>\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How many models should we bag in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- There is general no magic number. \n",
    "\n",
    "- The smaller the dataset, the less we could trust in the faithfulness of a random validation portion of it to represent the underlying phenomenon that generated the data, and hence we might wish to ensemble more of them to help average our poorly performing models resulting from bad splits of the data.\n",
    "\n",
    "- Usually practical considerations like computation power as well as dataset size determine if bagging is used and if so how many models are employed in the average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging vs. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Bagging and boosting are both \"ensembling\" methods, as they are used to ensemble or combine different models to improve efficacy. \n",
    "\n",
    "- With <u><b>boosting</b></u> we build up a <u><b>single</b></u> cross-validated model by gradually <u><b>adding</b></u> together simple models consisting of a single universal approximator unit. Each of these units are trained in a way that makes each individual model <u><b>dependent</b></u> on its predecessors (that are trained first).\n",
    "\n",
    "- With <u><b>bagging</b></u> we <u><b>average</b></u> together <u><b>multiple</b></u> models that have been trained <u><b>independently</b></u> of each other. Indeed any one of those cross-validated models in a bagged ensemble can itself be a boosted model."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "199px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
