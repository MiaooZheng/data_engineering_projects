{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Chapter 11: Principles of Feature Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 11.3 Universal Approximation of Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can toggle the code on and off in this presentation via the button below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the previous Section we saw how a nonlinear model built from units of a single universal approximator can be made to tightly approximate any *perfect dataset* if we increase its capacity sufficiently and tune the model's parameters properly by minimizing an appropriate cost function. In this Section we will investigate how universal approximation carries over to the case of *real data*, i.e., data that is finite in size and potentially noisy. We will then learn about a new measurement tool, called *validation error*, that will allow us to effectively employ universal approximators with real data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## This code cell will not be shown in the HTML version of this notebook\n",
    "# imports from custom library\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import autograd.numpy as np\n",
    "from mlrefined_libraries import nonlinear_superlearn_library as nonlib\n",
    "datapath = '../../mlrefined_datasets/nonlinear_superlearn_datasets/'\n",
    "\n",
    "# plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# this is needed to compensate for %matplotlib notebook's tendancy to blow up images when plotted inline\n",
    "%matplotlib notebook\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prototypical examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Here we explore the use of universal approximators in representing real data using two simple examples: a regression and two-class classification dataset. The problems we encounter with these two simple examples mirror those we face in general when employing universal approximator-based models to real data, regardless of problem type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example.</span>  Universal approximation of real regression data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In this example we illustrate the use of universal approximators on a real regression dataset that is based on the near-perfect sinusoidal data presented in Example 11.5. To simulate a real version of this dataset we randomly selected $P = 21$ of its points and added a small amount of random noise to the output (i.e., $y$ component) of each point, as illustrated in [Figure 11.18](#figure-11-18)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "(left panel) A *near-perfect* sinusoidal dataset. (right panel) A *real* regression dataset formed by adding random noise to the output of a small subset of the dataset on the right.\n",
    "\n",
    "<p>\n",
    "  <img src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_18.png' width=\"85%\"  alt=\"\"/>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the following animation we illustrate the fully tuned nonlinear fit of a model employing $1$ through $20$ polynomial, neural network, and stump units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"700\" height=\"400\" controls loop>\n",
       "  <source src=\"videos/animation_2.mp4\" type=\"video/mp4\">\n",
       "  </video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This code cell will not be shown in the HTML version of this notebook\n",
    "# produce visual comparison of different universal approximator fits\n",
    "demo = nonlib.regression_basis_comparison_2d.Visualizer()\n",
    "csvname = datapath + 'noisy_sin_sample.csv'\n",
    "demo.load_data(csvname)\n",
    "demo.brows_fits(savepath = 'videos/animation_2.mp4',num_elements = [v for v in range(1,21,1)],scatter = 'on',fps=2)\n",
    "\n",
    "# load in video and display\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"700\" height=\"400\" controls loop>\n",
    "  <source src=\"videos/animation_2.mp4\" type=\"video/mp4\">\n",
    "  </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice how:\n",
    "\n",
    "- with each of the universal approximators, all three models *underfit* the data when using only $B = 1$ unit in each case.\n",
    "\n",
    "- each model improves as we increase $B$, but only up to a certain point after which each *tuned* model becomes far too complex and overfits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following animation shows several polynomial-based models along with the corresponding Least Squares cost value each attains, i.e., the model's *training error*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"800\" height=\"400\" controls loop>\n",
       "  <source src=\"videos/animation_3.mp4\" type=\"video/mp4\">\n",
       "  </video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code cell will not be shown in the HTML version of this notebook\n",
    "demo2 = nonlib.regression_basis_single.Visualizer()\n",
    "csvname = datapath + 'noisy_sin_sample.csv'\n",
    "demo2.load_data(csvname)\n",
    "demo2.browse_single_fit(savepath='videos/animation_3.mp4',basis='poly',num_units = [v for v in range(1,21,1)],fps=1)\n",
    "\n",
    "# load in video and display\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"800\" height=\"400\" controls loop>\n",
    "  <source src=\"videos/animation_3.mp4\" type=\"video/mp4\">\n",
    "  </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice how:\n",
    "\n",
    "- in adding more polynomial units we turn up the capacity of our model and, optimizing each model to completion, the resulting tuned models achieve lower and lower training error. \n",
    "\n",
    "- the resulting fit provided by each fully tuned model (after a certain point) becomes far too complex and starts to get *worse* in terms of how it represents the general regression phenomenon.\n",
    "\n",
    "- as a measurement tool the training error only tells us how well a tuned model fits the *training data*, but fails to tell us when our tuned model becomes too complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example.</span>  Universal approximation of real classification data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In this example we illustrate the application of universal approximator-based models on a real two-class classification dataset that is based on the near-perfect dataset presented in Example 11.6. Here we simulated a realistic version of this data by randomly selecting $P = 99$ of its points, and adding a small amount of classification noise by flipping the labels of $5$ of those points, as shown in Figure 11.18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "(left panel) A near-perfect classification dataset. (right panel) A real dataset formed from a noisy subset of these points.\n",
    "\n",
    "<p>\n",
    "  <img src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_21.png' width=\"85%\"  alt=\"\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the following animation we show the nonlinear decision boundaries provided by fully tuned models employing polynomial, neural network, and stump units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"800\" height=\"400\" controls loop>\n",
       "  <source src=\"videos/animation_4.mp4\" type=\"video/mp4\">\n",
       "  </video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code cell will not be shown in the HTML version of this notebook\n",
    "# load in data\n",
    "csvname = datapath + 'new_circle_data.csv'\n",
    "demo3 = nonlib.main_classification_comparison.Visualizer(csvname)\n",
    "\n",
    "# run approximators\n",
    "demo3.runs1 = demo3.run_poly(5)\n",
    "demo3.runs2 = demo3.run_net(12)\n",
    "demo3.runs3 = demo3.run_trees(20)\n",
    "\n",
    "# animate\n",
    "frames = 5\n",
    "demo3.animate_comparisons(savepath = 'videos/animation_4.mp4',frames=frames,fps=1)\n",
    "\n",
    "# load in video and display\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"800\" height=\"400\" controls loop>\n",
    "  <source src=\"videos/animation_4.mp4\" type=\"video/mp4\">\n",
    "  </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice how:\n",
    "\n",
    "- at the start all three tuned models are not complex enough and thus *underfit* the data, providing a classification that in all instances simply classifies the entire space as belonging to the blue class.\n",
    "\n",
    "- each model improves as more units are added, with $B = 5$ polynomial units, $B = 3$ neural network units, and $B = 5$ stump units providing reasonable approximations to the desired circular decision boundary.\n",
    "\n",
    "- soon after we reach these numbers of units each tuned model becomes too complex and *overfits* the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the following animation we plot several neural network-based models along with the corresponding two-class Softmax cost value each attains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"800\" height=\"400\" controls loop>\n",
       "  <source src=\"videos/animation_5.mp4\" type=\"video/mp4\">\n",
       "  </video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code cell will not be shown in the HTML version of this notebook\n",
    "import copy\n",
    "\n",
    "# the mid-point between each set of consecutive inputs along each input dimension\n",
    "csvname = datapath + 'new_circle_data.csv'\n",
    "data = np.loadtxt(csvname,delimiter = ',')\n",
    "x = data[:-1,:]\n",
    "y = data[-1:,:]\n",
    "\n",
    "### boosting ###\n",
    "num_units = 6\n",
    "run1 = []\n",
    "for j in range(num_units):\n",
    "    print ('fitting ' + str(j + 1) + ' units')\n",
    "    \n",
    "    # import the v1 library\n",
    "    mylib1 = nonlib.intro_general_library.superlearn_setup.Setup(x,y)\n",
    "\n",
    "    # choose features\n",
    "    mylib1.choose_features(name = 'multilayer_perceptron',layer_sizes = [2,j+1,1],activation = 'tanh')\n",
    "\n",
    "    # choose normalizer\n",
    "    mylib1.choose_normalizer(name = 'standard')\n",
    "\n",
    "    # pick training set\n",
    "    mylib1.make_train_valid_split(train_portion=1)\n",
    "            \n",
    "    # choose cost\n",
    "    mylib1.choose_cost(name = 'softmax')\n",
    "\n",
    "    # fit an optimization\n",
    "    mylib1.fit(max_its = 6000,alpha_choice = 10**(0),optimizer = 'gradient_descent')\n",
    "\n",
    "    # add model to list\n",
    "    run1.append(copy.deepcopy(mylib1))\n",
    "    \n",
    "# animate the business\n",
    "frames = num_units\n",
    "demo = nonlib.main_classification_animators.Visualizer(csvname)\n",
    "savepath = 'videos/animation_5.mp4'\n",
    "demo.animate_classifications(savepath,run1,frames,fps=0.5)\n",
    "\n",
    "# load in video and display\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"800\" height=\"400\" controls loop>\n",
    "  <source src=\"videos/animation_5.mp4\" type=\"video/mp4\">\n",
    "  </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice how: \n",
    "\n",
    "- increasing model capacity by adding more units always *decreases* the cost function value (just as with perfect or near-perfect data).\n",
    "\n",
    "- the resulting classification, after a certain point, actually gets *worse* in terms of how it (the learned decision boundary) represents the general classification phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In summary, in the Examples here we have seen that, unlike the case with perfect data, when employing universal approximator-based models with real data we must be careful with how we set the capacity of our model, as well as how well we tune its parameters via optimization of an associated cost. These two simple examples also show how training error fails as a reliable tool to measure how well a tuned model represents the phenomenon underlying a real dataset. Both of these issues arise in general, and are discussed in greater detail next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The capacity and optimization dials, revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- When dealing with perfect data the best setting of capacity/optimization dials is when both dials are set all the way to the right.\n",
    "\n",
    "- This means employing a model with maximum possible capacity and minimizing its corresponding cost to completion.\n",
    "\n",
    "- The prototypical examples we just saw illustrate how with real data we cannot simply set our capacity and optimization dials all the way to the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To gain a stronger intuition for this principle, suppose first that we set our optimization dial all the way to the right (meaning that regardless of the dataset and model we use, we always tune its parameters by minimizing the corresponding cost function to completion). Then with perfect data, as illustrated in the top row of [Figure 11.24](#figure-11-24),  as we turn our capacity dial from left to right (e.g., by adding more units) the resulting tuned model provides a better and better representation of a the perfect dataset.\n",
    "\n",
    "However with real data, as illustrated in the bottom row of [Figure 11.24](#figure-11-24), starting with our capacity dial all the way to the left, the resulting tuned model is not complex enough for the phenomenon underlying our data.  We say that such a tuned model *underfits*, as it does not even fit the given data well.  Notice that while the simple visual depiction here illustrates an underfitting model as a linear ('un-wiggly') function - which is quite common in practice - it is possible for an underfitting model to be quite 'wiggly'.  Regardless of the 'shape' a tuned model takes we say that it underfits if it poorly represents the training data - i.e., if it has high training error.\n",
    "\n",
    "Turning the capacity dial from left to right *increases* the complexity of each tuned model, providing a better and better representation of the data and the phenomenon underlying it. However there comes a point, as we continue turning the dial from left to right, where the corresponding tuned model becomes *too complex* to represent the phenomenon underlying the data. Indeed past this point, where the complexity of each tuned model is wildly inappropriate for the phenomenon at play, we say that *overfitting* begins. This language is used because while such highly complex models fit the given data extremely well, they do so at the cost of not representing the underlying phenomenon well. As we continue to turn our capacity dial to the right the resulting tuned models will become increasingly complex and increasingly less representative of the true underlying phenomenon.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Setting the optimization dial all the way to the right, with real data (unlike perfect data) we cannot simply set our capacity dial all the way to the right as well! \n",
    "<p>\n",
    "  <img src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_24.png' width=\"55%\"  alt=\"\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now suppose instead that we turn our capacity dial all the way to the right, using a very high capacity model, and set its parameters by turning our optimization dial ever so slowly from left to right. In the case of perfect data, as illustrated in the top row of [Figure 11.25](#figure-11-25), this approach produces tuned models that increasingly represents the data well. With real data on the other hand, as illustrated in the bottom row of [Figure 11.25](#figure-11-25), starting with our optimization dial set all the way to the left will tend to produce low complexity *underfitting* tuned models. As we turn our optimization dial from left to right, taking steps of a particular local optimization scheme, our corresponding model will tend to increase in complexity, improving its representation of the given data. This improvement continues only up to a point where our corresponding tuned model becomes too complex for the phenomenon underlying the data, and hence *overfitting* begins. After this point the tuned models arising from turning the optimization dial further to the right are far too complex to adequately represent the phenomenon underlying a dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Setting the capacity dial all the way to the right, with real data (unlike perfect data) we cannot simply set our optimization dial all the way to the right as well!\n",
    "\n",
    "<p>\n",
    "  <img src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_25.png' width=\"55%\"  alt=\"\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivating a new measurement tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How we set our capacity and optimization dials in order to achieve a final tuned model that has *just the right amount of complexity* for a given dataset is the main challenge we face when employing universal approximator-based models with real data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "In Examples 11.7 and 11.8 we saw how training error fails to indicate when a tuned model has sufficient *complexity* for the tasks of regression and two-class classification respectively, a fact more generally true about all nonlinear machine learning problems as well. If we cannot rely on training error to help decide on the proper amount of complexity required to address real nonlinear machine learning tasks, what sort of measurement tool should we use instead? Closely examining [Figure 11.26](#figure-11-26) reveals the answer! \n",
    "\n",
    "In the top row of this Figure we show three instances of models presented for the toy regression dataset in Example 11.6: a fully tuned low complexity (and underfitting) linear model in the left panel, a high complexity (and overfitting) degree $20$ polynomial model in the middle panel, and a degree $3$ polynomial model in the right panel that fits the data and the underlying phenomenon generating it \"just right.\" What do both the underfitting and overfitting patterns have in common, that the \"just right\" model does not?\n",
    "\n",
    "Scanning the left two panels of the Figure we can see that a common \n",
    "problem with both the underfitting and overfitting models is that, while they differ in how well they represent data *we already have*, they will both fail to adequately represent *new data* generated via the same process by which the current data was made. In other words, we would not trust either model to predict the output of a newly arrived input point. The \"just right\" fully tuned model does not suffer from the same problem as it closely approximates the sort of wavy sinusoidal pattern underlying the data, and as a result would work well as a predictor for future data points.\n",
    "\n",
    "The same story tells itself in the bottom row of the Figure with our two-class classification dataset used previously in Example 11.7. Here we show a fully tuned low complexity (and underfitting) linear model in the left panel, a high complexity (and overfitting) degree $20$ polynomial model in the middle panel, and \"just right\" degree $2$ polynomial model in the right panel. As with the regression case, the underfitting and overfitting models both fail to adequately\n",
    "represent the underlying phenomenon generating our current data and, as a result, will fail to adequately predict the label values of *new data* generated via the same process by which the current data was made. \n",
    "\n",
    "In summary, with both the simple regression and classification examples discussed here we can roughly qualify poorly performing models as those that will not allow us to make accurate predictions of data we will receive in the future. But how do we quantify something we will receive in the future? We address this next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What do both the underfitting and overfitting patterns have in common, that the \"just right\" model does not?\n",
    "\n",
    "<p>\n",
    "  <img src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_26.png' width=\"65%\"  alt=\"\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Observation\n",
    "\n",
    "While the underfitting and overfitting models differ in how well they represent data *we already have*, they will both fail to adequately represent *new data* generated via the same process by which the current data was made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question\n",
    "\n",
    "But how do we quantify something we will receive in the future?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The validation error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To simulate the data we receive in the future, we simply remove a random portion of our data and treat it as \"new data we might receive in the future.\" We train our selection of models on only the portion of data that remains, and *validate* the performance of each model on this randomly removed chunk of \"new\" data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The random portion of the data we remove to validate our model(s) is commonly called the *validation data* \n",
    "- The remaining portion we use to train models is likewise referred to as the *training data*\n",
    "\n",
    "<p>\n",
    "  <img src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_27.png' width=\"85%\"  alt=\"\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The model providing the lowest error on the validation data, i.e., the *lowest validation error*, is then deemed the best choice from a selection of trained models. As we will see validation error (unlike training error) is in fact a proper measurement tool for determining the quality of a model against the underlying data generating phenomenon we want to capture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- There is no precise rule for what portion of a given dataset should be saved for validation. \n",
    "\n",
    "- In practice, typically between $\\frac{1}{10}$ to $\\frac{1}{3}$ of the data is assigned to the validation set.\n",
    "\n",
    "- Generally speaking, the larger and/or more representative (of the true phenomenon from which the data is sampled) a dataset is the larger the portion of the original data may be assigned to the validation set (e.g., $\\frac{1}{3}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The intuition for doing this is that if the data is plentiful/representative enough, the training set still accurately represents the underlying phenomenon even after removal of a relatively large set of validation data. Conversely, in general with smaller or less representative (i.e., more noisy or poorly distributed) datasets we usually take a smaller portion for validation (e.g., $\\frac{1}{10}$) since the relatively larger training set needs to retain what little information of the underlying phenomenon was captured by the original data, and little data can be spared for validation."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "160.533px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
