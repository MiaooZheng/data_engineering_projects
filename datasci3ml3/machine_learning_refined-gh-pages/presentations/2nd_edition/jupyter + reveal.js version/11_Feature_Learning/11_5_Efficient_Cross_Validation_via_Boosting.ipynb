{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Chapter 11: Principles of Feature Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 11.5 Efficient Cross-Validation via Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can toggle the code on and off in this presentation via the button below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "With *boosting based cross validation* we perform our model search by taking a *single* high capacity model and optimize it *one unit at-a-time*, resulting in a much more efficient high resolution cross-validation procedure than the naive form of cross-validation.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## This code cell will not be shown in the HTML version of this notebook\n",
    "# imports from custom library\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import autograd.numpy as np\n",
    "from mlrefined_libraries import nonlinear_superlearn_library as nonlib\n",
    "datapath = '../../mlrefined_datasets/nonlinear_superlearn_datasets/'\n",
    "\n",
    "# plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# this is needed to compensate for %matplotlib notebook's tendancy to blow up images when plotted inline\n",
    "%matplotlib notebook\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The big picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The basic principle behind boosting based cross-validation is to progressively build a high capacity model <b><u>one unit at-a-time</u></b>, using units from a single type of universal approximator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "as\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{model}\\left(\\mathbf{x},\\Theta\\right) = w_0 + f_1\\left(\\mathbf{x}\\right){w}_{1} +  f_2\\left(\\mathbf{x}\\right){w}_{2} + \\cdots + f_M\\left(\\mathbf{x}\\right)w_M.\n",
    "\\label{equation:boosting-original-construct}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We do this sequentially in $M$ 'rounds' of boosting where at each round we add one unit to the model, completely optimizing this unit's parameters alone along with its corresponding linear combination weight (we keep these parameters fixed at these optimally tuned values forever more).\n",
    "\n",
    "Alternatively we can think of this procedure as beginning with a high capacity model of the form above and - in $M$ rounds - optimizing the parameters of each unit one unit at-a-time (a form of coordinate-wise optimization). \n",
    "\n",
    "In either case, performing boosting in this way produces a sequence of $M$ tuned models that generally increase in complexity with respect to the training dataset, which we denote compactly as $\\left\\{\\text{model}_m\\right\\}_{m=1}^M$, where the $m^{th}$ model consists of $m$ tuned units (each having been tuned one at-a-time in the preceding rounds).\n",
    "\n",
    "Since just one unit is optimized at-a-time, boosting tends to provide a computationally efficient high resolution form of model search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The general boosting procedure tends to produce training/validation error curves that generally look like those shown in the top panel of [Figure 11.32](#figure-11-32).  As with the naive approach detailed in the previous Section, here too we tend to see training error decrease as $m$ grows larger while validation error tends to start high where underfitting occurs, dip down to a minimum value (perhaps oscillating more than the one time illustrated here), and rise back up when overfitting begins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img align=\"right\" src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_32.png' width=\"68%\"  alt=\"\"/>\n",
    "<b>(top panel)</b> Prototypical training and validation error curves associated with a completed run of boosting.<br><br> <b>(bottom panels)</b> We fix the capacity dial all the way to the right and the optimization dial all the way to the left, slowly turning it from left to right, with each notch denoting the complete optimization of one additional unit of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Using the capacity/optimization dial conceptualization first introduced in Section 11.2.4, we can think about boosting as starting with our *capacity dial* set all the way to the *right* at some high value (e.g., some large value of $M$), and fidgeting with the *optimization dial* by turning it very slowly from left to right starting at the far left (as depicted in the bottom row of Figure 11.15).  As discussed in Section 11.3.2 and summarized visually in the bottom row of Figure 11.25, with real data this general configuration (setting the capacity dial to the right, and adjusting optimization dial) allows our *optimization dial* to (roughly speaking) govern the complexity of a tuned model based on how well we optimize (the more we optimize a high capacity model the higher its complexity with respect to the training data becomes).  In other words with this configuration our optimization dial (roughly speaking) becomes the sort of fine resolution *complexity* dial we aimed to construct at the outset of the Chapter (see Section 11.1).  With our optimization dial turned all the way to the left we begin our search with a low complexity tuned model (called $\\text{model}_1$) consisting of a single unit of a universal approximator having its parameters fully optimized.  As we progress through rounds of boosting we turn the optimization dial gradually from left to right (here each notch on the optimization dial denotes the complete optimization of one additional unit) optimizing (to completion) a single weighted unit of our original high capacity model, so that at the $m^{th}$ round our tuned model (called $\\text{model}_m$) consists of $m$ individually but fully tuned units.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Our ultimate aim in doing this is of course to determine a setting of the optimization dial / determine an appropriate number of tuned units that minimizes validation error.  We visualize this concept in [Figure 11.33](#figure-11-33) by wrapping the prototypical validation error curve (shown in its top panel of this Figure) around the optimization dial (shown in its bottom right panel) from left to right, as well as the generic markings denoting underfitting and overfitting.  By using validation error we automate the process of setting our optimization dial to the proper setting - where validation error is minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Whether we use fixed-shape, neural network, or tree-based units with boosting we will naturally prefer units with *low capacity* so that the resolution of our model search is as fine-grained as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<b>(left panel)</b> Using <u>low-capacity units</u> makes the boosting procedure a high (or fine) resolution search for optimal model complexity. <b>(right panel)</b> Using <u>high-capacity units</u> makes boosting a low (or coarse) resolution search for optimal model complexity.<br>\n",
    "<p>\n",
    "  <img src= '../../mlrefined_images/nonlinear_superlearn_images/Figure_11_33.png' width=\"100%\"  alt=\"\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is depicted in the left panel of [Figure 11.33](#figure-11-33). If we used *high capacity* units at each round of boosting the resulting model search will be much coarser, as adding each additional unit results in aggressively turning the dial from left to right leaving large gaps in our model search, as depicted in the right panel of [Figure 11.33](#figure-11-33).   This kind of low resolution search could easily result in us skipping over the complexity of an optimal model.  The same can be said as to why we add only one unit at-a-time with boosting, tuning its parameters alone at each round. If we added more than one unit at-a-time, or if we re-tuned *every* parameter of *every* unit at each step of this process not only would we have significantly more computation to perform at each step but the performance difference between subsequent models could be quite large and we might easily miss out on an ideal model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Technical details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For boosting we need a set of $M$ nonlinear features or units from a single family of universal approximators\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{F} = \\{f_{1}\\left(\\mathbf{x}\\right),\\,f_{2}\\left(\\mathbf{x}\\right),\\,\\ldots,\\,f_M\\left(\\mathbf{x}\\right)\\}.\n",
    "\\end{equation}\n",
    "\n",
    "We add these units sequentially or one-at-a-time building a set of $M$ tuned models $\\left[\\text{model}_m\\right]_{m=1}^M$ that increase in complexity with respect to the training data, from $m=1$ to $m=M$, ending with a generic nonlinear model composed of $M$ units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will express this final boosting-made model as\n",
    "\n",
    "$$\n",
    "\\text{model}\\left(\\mathbf{x},\\Theta\\right) = w_0 + f_{s_1}\\left(\\mathbf{x}\\right){w}_{1} +  f_{s_2}\\left(\\mathbf{x}\\right){w}_{2} + \\cdots + f_{s_M}\\left(\\mathbf{x}\\right)w_{M}.\n",
    "\\label{equation:final-boosting-model}$$\n",
    "\n",
    "Here we have re-indexed the individual units to $f_{s_m}$ (and the corresponding weight $w_{m}$) to denote the unit from the entire collection in $\\mathcal{F}$ added at the $m^{th}$ round of the boosting\n",
    "process. The linear combination weights $w_0$ through $w_M$ along with any additional\n",
    "weights internal to $f_{s_1},\\,f_{s_2},\\,\\ldots,\\,f_{s_M}$ are\n",
    "represented collectively in the weight set $\\Theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The process of boosting is performed in a total of $M$ rounds.\n",
    "- At each round we determine which unit, when added to the running model, best lowers its training error.\n",
    "- We then measure the corresponding validation error provided by this update, and in the end after all rounds of boosting are complete, use the lowest validation error measurement found to decide which round provided the best overall model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- For the sake of simplicity, we only discuss nonlinear regression on the training dataset $\\left\\{\\left(\\mathbf{x}_p,\\,y_p\\right)\\right\\}_{p=1}^P$ employing the Least Squares cost.\n",
    "\n",
    "- However, the principles of boosting we will see remain *exactly* the same for other learning tasks (e.g., two-class and multi-class classification) and their associated costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Round 0 of boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Round 0 starts with the model \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{model}_0^{\\,}\\left(\\mathbf{x},\\Theta_0\\right) = w_0^{\\,}\n",
    "\\end{equation}\n",
    "\n",
    "whose weight set $\\Theta_0^{\\,} = \\left\\{ w_0\\right\\}$ contains a\n",
    "single bias weight, whose optimal value $w_0^{\\star}$ is found by by minimizing the Least Squares cost\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{P}\\sum_{p=1}^{P}\\left(\\text{model}_0^{\\,}\\left(\\mathbf{x}_p,\\Theta_0 \\right)  - \\overset{\\,}{y}_{p}^{\\,}\\right)^{2} =  \\frac{1}{P}\\sum_{p=1}^{P}\\left(w_0^{\\,}   - \\overset{\\,}{y}_{p}^{\\,}\\right)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "We fix the bias weight at the value of $w_0^{\\star}$ forever more throughout the\n",
    "process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Round 1 of boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "Having tuned the only parameter of $\\text{model}_0$ we now *boost* its complexity by adding the weighted unit\n",
    "$f_{s_1}\\left(\\mathbf{x}\\right)w_{1}$ to it:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{model}_1^{\\,}\\left(\\mathbf{x},\\Theta_1^{\\,}\\right) = \\text{model}_0^{\\,}\\left(\\mathbf{x},\\Theta_0^{\\,}\\right) + f_{s_1}\\left(\\mathbf{x}\\right)w_{1}^{\\,}.\n",
    "\\end{equation}\n",
    "\n",
    "To determine which unit in our set $\\mathcal{F}$ best lowers the training error, we press\n",
    "$\\text{model}_1$ against the data by minimizing the following cost for every unit $f_{s_1} \\in \\mathcal{F}$\n",
    "\n",
    "\\begin{equation}\n",
    " \\frac{1}{P}\\sum_{p=1}^{P}\\left(\\text{model}_0^{\\,}\\left(\\mathbf{x}_p,\\Theta_0^{\\,}\\right) + f_{s_1}\\left(\\mathbf{x}_p^{\\,}\\right)w_1   - {y}_{p}^{\\,}\\right)^{2} \\\\=  \\frac{1}{P}\\sum_{p=1}^{P}\\left(w_0^{\\star} + f_{s_1}\\left(\\mathbf{x}_p\\right)w_1  - \\overset{\\,}{y}_{p}^{\\,}\\right)^{2}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Round $m>1$ of boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We begin with $\\text{model}_{m-1}$ consisting of a bias term and $m-1$ units of the form\n",
    "    \n",
    "$$\n",
    "\\text{model}_{m-1}\\left(\\mathbf{x},\\Theta_{m-1}\\right) = w_0^{\\star} + f_{s_1}^{\\star}\\left(\\mathbf{x}\\right){w}_{1}^{\\star}  + \\cdots + f_{s_{m-1}}^{\\star}\\left(\\mathbf{x}\\right)w_{m-1}^{\\star}.\n",
    "\\label{equation:cv_boosting_model_m_minus_1}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We then seek out the best weighted unit \n",
    "$f_{s_m}\\left(\\mathbf{x}\\right)w_{m}$ to add to our running model\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{model}_m^{\\,}\\left(\\mathbf{x},\\Theta_m^{\\,}\\right) = \\text{model}_{m-1}^{\\,}\\left(\\mathbf{x},\\Theta_{m-1}^{\\,}\\right) + f_{s_m}\\left(\\mathbf{x}\\right)w_{m}\n",
    "\\label{equation:boosting-round-m-model-version-1}\n",
    "\\end{equation}\n",
    "\n",
    "by minimizing the following cost over $w_m$, $f_{s_m}$ and its internal parameters (if they exist)\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}\n",
    "\\\n",
    " \\frac{1}{P}\\sum_{p=1}^{P}\\left(\\text{model}_{m-1}^{\\,}\\left(\\mathbf{x}_p,\\Theta_{m-1}^{\\,}\\right) + f_{s_m}\\left(\\mathbf{x}_p^{\\,}\\right)w_m   - {y}_{p}^{\\,}\\right)^{2} = \\\\ \\frac{1}{P}\\sum_{p=1}^{P}\\left(w_0^{\\star} + w_1^{\\star}f^{\\star}_{s_{1}}  + \\cdots + f^{\\star}_{s_{m-1}}\\left(\\mathbf{x}_p\\right)w_{m-1}^{\\star} + f_{s_m}\\left(\\mathbf{x}_p\\right)w_m  - \\overset{\\,}{y}_{p}^{\\,}\\right)^{2}\n",
    " \\end{array}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note again the form of the following cost function which we have to minimize over $w_m$ and $f_{s_m}$: \n",
    "\n",
    "$$\n",
    "\\frac{1}{P}\\sum_{p=1}^{P}\\left(w_0^{\\star} + w_1^{\\star}f^{\\star}_{s_{1}}  + \\cdots + f^{\\star}_{s_{m-1}}\\left(\\mathbf{x}_p\\right)w_{m-1}^{\\star} + f_{s_m}\\left(\\mathbf{x}_p\\right)w_m  - \\overset{\\,}{y}_{p}^{\\,}\\right)^{2}\n",
    "$$\n",
    "\n",
    "- If we use <b><u>fixed-shape</u></b> or <b><u>tree-based</u></b> approximators, this entails solving $M$ (or $M-m+1$, if we decide to check only those units not used in previous rounds) such optimization problems and choosing the one with smallest training error.\n",
    "\n",
    "- If we use <b><u>neural networks</u></b>, since each unit takes the same form, we need only solve one such optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Once all rounds of boosting are complete note how we have generated a\n",
    "sequence of \\(M\\) tuned models - denoted $\\left[\\text{model}_m\\left(\\mathbf{x},\\Theta_m^{\\,}\\right)\\right]_{m=1}^M$ - \n",
    "which gradually increases in nonlinear complexity from $m = 1$ to\n",
    "$m = M$, and thus gradually decrease in training error. This gives us fine-grained control in selecting an appropriate model, as the jump in performance in terms of both the training and validation\n",
    "errors between subsequent models in this sequence can be quite smooth in the sequence, provided we use low capacity units (as discussed in Section 11.5.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Once boosting is complete we select from our set of models the one that provides the lowest validation error.\n",
    "\n",
    "- Alternatively, instead of running all rounds of boosting and deciding on an optimal model after the fact, we can attempt to *halt* the procedure when the validation error first starts to increase.\n",
    "\n",
    "- This concept, referred to as *early stopping*, leads to a more computationally efficient implementation of boosting.\n",
    "\n",
    "- However, one needs to be careful in deciding when the validation error has really reached its minimum as it can oscillate up and down multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An inexpensive but effective enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- At the $m^{th}$ round of boosting we can add an additional bias weight $w_{0, m}$ as\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{model}_m^{\\,}\\left(\\mathbf{x},\\Theta_m^{\\,}\\right) = \\text{model}_{m-1}^{\\,}\\left(\\mathbf{x},\\Theta_{m-1}^{\\,}\\right) + w_{0, m} + f_{s_m}\\left(\\mathbf{x}\\right)w_{m}.\n",
    "\\label{equation:boosting-round-m-model-version-2}\n",
    "\\end{equation}\n",
    "\n",
    "- This simple adjustment results in greater flexibility and generally better overall performance by allowing units to be adjusted 'vertically' at each round."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Note that once tuning is done, the optimal bias weight $w^{\\star}_{0, m}$ can be absorbed into the bias weights from previous\n",
    "rounds.\n",
    "\n",
    "- This enhancement is particularly useful when using fixed-shape or neural network units for boosting, and is redundant\n",
    "when using tree-based approximators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This enhancement is particularly useful when using fixed-shape or neural network units for boosting, as it is redundant\n",
    "when using tree-based approximators because they already have individual bias terms baked into them that always allow for this kind of vertical adjustment at each round of boosting (in the jargon of machine learning boosting with tree-based learners is often referred to as *gradient boosing* - see Section 14.5.  To see this note that while the most common way of expressing a stump taking in $N=1$ dimensional input is \n",
    "\n",
    "\\begin{equation}\n",
    "f\\left(x\\right)=\\begin{cases}\n",
    "\\begin{array}{c}\n",
    "v_{1}\\\\\n",
    "v_{2}\n",
    "\\end{array} & \\begin{array}{c}\n",
    "x<s \\\\\n",
    "x>s\n",
    "\\end{array}\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "it is also possible to express $f(x)$ equivalently as\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = b + w\\,h(x)\n",
    "\\label{equation:stump-chapter-14-bias-feature-weight-version}\n",
    "\\end{equation}\n",
    "\n",
    "where $b$ denotes an individual bias parameter for the stump and $w$ is an associated weight that scales $h(x)$, which is a simple step function with fixed levels and a split at $x=s$\n",
    "\n",
    "\\begin{equation}\n",
    "h\\left(x\\right)=\\begin{cases}\n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{array} & \\begin{array}{c}\n",
    "x<s\\\\\n",
    "x>s\n",
    "\\end{array}\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Expressing the stump in this equivalent manner allows us to see that every stump unit does indeed have its own individual bias parameter, making it redundant to add an individual bias at each round when boosting with stumps (and the same holds for stumps taking in general $N$ dimensional input as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example.</span>  Boosting regression using stump units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In the following animation we illustrate the result of $M = 100$ rounds of boosting using a set of $B = 20$ stumps (many of the stumps are used multiple times) on a toy regression dataset.\n",
    "\n",
    "- Data is split into $\\frac{2}{3}$ training (blue) and $\\frac{1}{3}$ validation (yellow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"800\" height=\"400\" controls loop>\n",
       "  <source src=\"videos/animation_8.mp4\" type=\"video/mp4\">\n",
       "  </video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This code cell will not be shown in the HTML version of this notebook\n",
    "# import data\n",
    "csvname = datapath + 'noisy_sin_sample.csv'\n",
    "data = np.loadtxt(csvname,delimiter = ',')\n",
    "x = data[:-1,:]\n",
    "y = data[-1:,:] \n",
    "\n",
    "# import booster\n",
    "mylib = nonlib.intro_boost_library.stump_booster.Setup(x,y)\n",
    "\n",
    "# choose normalizer\n",
    "mylib.choose_normalizer(name = 'standard')\n",
    "\n",
    "# pick training set\n",
    "mylib.make_train_valid_split(train_portion=0.66)\n",
    "\n",
    "# choose cost|\n",
    "mylib.choose_cost(name = 'least_squares')\n",
    "\n",
    "# choose optimizer\n",
    "mylib.choose_optimizer('newtons_method',max_its=1)\n",
    "\n",
    "# run boosting\n",
    "mylib.boost(51)\n",
    "\n",
    "# produce animation\n",
    "csvname = datapath + 'noisy_sin_sample.csv'\n",
    "frames = 50\n",
    "anim = nonlib.boosting_regression_animators.Visualizer(csvname)\n",
    "savepath = 'videos/animation_7.mp4'\n",
    "anim.animate_trainval_boosting(savepath,mylib,frames,fps=5)\n",
    "\n",
    "# load in video and display\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"800\" height=\"400\" controls loop>\n",
    "  <source src=\"videos/animation_8.mp4\" type=\"video/mp4\">\n",
    "  </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 11.5.5 Similarity to feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The careful reader will notice how similar the boosting procedure is to the one introduced in [Section 9.6](https://jermwatt.github.io/machine_learning_refined/notes/9_Feature_engineer_select/9_6_Boosting.html) in the context of feature selection. Indeed principally the two approaches are entirely similar, except with boosting we do not select from a set of given input features but create them ourselves based on a chosen universal approximator family. Additionally, unlike feature selection where our main concern is *human interpret-ability*, we primarily use boosting as a tool for cross-validation. This means that unless we specifically prohibit it from occurring, we can indeed select the same feature multiple times in the boosting process as long as it contributes positively towards finding a model with minimal validation error.\n",
    "\n",
    "These two use-cases for boosting, i.e., feature selection and\n",
    "cross-validation, can occur together, albeit typically in the context\n",
    "of linear modeling as detailed in [Section 9.6](https://jermwatt.github.io/machine_learning_refined/notes/9_Feature_engineer_select/9_6_Boosting.html). Often in such instances\n",
    "cross-validation is used with a linear model as a way of automatically\n",
    "selecting an appropriate number of features, with human interpretation of the resulting selected features still in mind. On the other hand, rarely is feature selection done when employing a nonlinear model based on features from a universal approximator due to the great difficulty in the human interpret-ability of nonlinear features. The rare exception to this rule is when using tree-based units which, due to their simple structure, can in particular instances be readily interpreted by humans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The residual perspective with regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consider the following Least Squares cost function where we have inserted a boosted\n",
    "model at the $m^{th}$ round of its development\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(\\Theta_m^{\\,}\\right) = \\frac{1}{P}\\sum_{p=1}^{P}\\left(\\text{model}_m^{\\,}\\left(\\mathbf{x}_p,\\Theta_m^{\\,}\\right) - y_p\\right)^2.\n",
    "\\label{equation:boosting-LS-cost}\n",
    "\\end{equation}\n",
    "\n",
    "We can write our boosted model recursively as\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{model}_m^{\\,}\\left(\\mathbf{x}_p^{\\,},\\Theta_m^{\\,}\\right) = \\text{model}_{m-1}^{\\,}\\left(\\mathbf{x}_p^{\\,},\\Theta_{m-1}^{\\,}\\right) + f_m^{\\,}\\left(\\mathbf{x}_p\\right)w_m^{\\,}\n",
    "\\label{equation:boosting-LS-recursive-model}\n",
    "\\end{equation}\n",
    "\n",
    "where all of the parameters of the $\\left(m-1\\right)^{th}$ model (i.e., $\\text{model}_{m-1}$) are already tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Combining the equations of the previous page we can re-write the Least Squares cost as\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(\\Theta_m^{\\,}\\right) = \\frac{1}{P}\\sum_{p=1}^{P}\\left(f_m^{\\,}\\left(\\mathbf{x}_p^{\\,}\\right)w_m^{\\,} - \\left(y_p^{\\,} - \\text{model}_{m-1}^{\\,}\\left(\\mathbf{x}_p^{\\,}\\right)\\right)\\right)^2.\n",
    "\\end{equation}\n",
    "\n",
    "By minimizing this cost we look to tune the parameters of a\n",
    "single additional unit so that for all $p$ we have\n",
    "\n",
    "\\begin{equation}\n",
    "f_m^{\\,}\\left(\\mathbf{x}_p\\right)w_m^{\\,}\\approx y_p^{\\,} - \\text{model}_{m-1}^{\\,}\\left(\\mathbf{x}_p^{\\,}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "The RHS is the difference between our original output and the contribution of the $\\left(m-1\\right)^{th}$ model, is often called the *residual*: it is what is left to represent after subtracting off what was learned by the $\\left(m-1\\right)^{th}$ model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example.</span>  Boosting regression and the 'fitting to the residual' perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the animation below we illustrate the process of boosting $M = 5000$ single-layer $\\texttt{tanh}$ units to a toy regression dataset.\n",
    "\n",
    "- **left panel** shows the dataset along with the fit provided by $\\text{model}_m$ at the $m^{th}$ step of boosting for select values of $m$.\n",
    "- **right panel** shows the *residual* at the same step, as well as the fit provided by the corresponding $m^{th}$ unit $f_m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"800\" height=\"400\" controls loop>\n",
       "  <source src=\"videos/animation_9.mp4\" type=\"video/mp4\">\n",
       "  </video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This code cell will not be shown in the HTML version of this notebook\n",
    "import copy\n",
    "# load in dataset\n",
    "csvname = datapath + 'noisy_sin_sample.csv'\n",
    "data = np.loadtxt(csvname,delimiter = ',')\n",
    "x = copy.deepcopy(data[:-1,:])\n",
    "y = copy.deepcopy(data[-1:,:] )\n",
    "\n",
    "# boosting procedure\n",
    "num_units = 15\n",
    "runs2 = []\n",
    "for j in range(num_units):    \n",
    "    # import the v1 library\n",
    "    mylib2 = nonlib.intro_boost_library.net_booster.Setup(x,y)\n",
    "    \n",
    "    # choose normalizer\n",
    "    mylib2.choose_normalizer(name = 'standard')\n",
    "\n",
    "    # choose normalizer\n",
    "    mylib2.make_train_valid_split(train_portion = 1)\n",
    "\n",
    "    # choose cost\n",
    "    mylib2.choose_cost(name = 'least_squares')\n",
    "\n",
    "    # choose optimizer\n",
    "    mylib2.choose_optimizer('gradient_descent',max_its=10000,alpha_choice = 10**(-1))\n",
    "    \n",
    "    # choose activation \n",
    "    mylib2.choose_activation(activation = 'relu')\n",
    "    \n",
    "    # run boosting\n",
    "    mylib2.boost(1,verbose=False)\n",
    "    mylib2.model = mylib2.models[-1]\n",
    "\n",
    "    # add model to list\n",
    "    runs2.append(copy.deepcopy(mylib2))\n",
    "    \n",
    "    # cut off output given model\n",
    "    normalizer = mylib2.normalizer\n",
    "    ind = np.argmin(mylib2.train_cost_vals[0])\n",
    "    y_pred =  mylib2.models[-1](mylib2.normalizer(x))\n",
    "    y -= y_pred\n",
    "\n",
    "# animate the business\n",
    "frames = num_units\n",
    "demo2 = nonlib.boosting_regression_animators_v3.Visualizer(csvname)\n",
    "savepath='videos/animation_9.mp4'\n",
    "demo2.animate_boosting(savepath,runs2,frames,fps=2)\n",
    "\n",
    "# load in video and display\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"800\" height=\"400\" controls loop>\n",
    "  <source src=\"videos/animation_9.mp4\" type=\"video/mp4\">\n",
    "  </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "143px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
