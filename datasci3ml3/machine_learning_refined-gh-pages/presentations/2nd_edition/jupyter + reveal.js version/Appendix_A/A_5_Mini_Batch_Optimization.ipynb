{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A.5 Mini-Batch Optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In machine learning applications we almost never tasked with minimizing a single mathematical function, but one that consist a *sum* of $P$ functions.  \n",
    "\n",
    "\n",
    "- In other words the sort of function $g$ we very often need to minimize in machine learning applications takes the general form \n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(\\mathbf{w}\\right) = \\sum_{p=1}^P g_p\\left(\\mathbf{w}\\right).\n",
    "\\end{equation}\n",
    "\n",
    "where $g_1,\\,g_2,\\,...,g_P$ are mathematical functions themselves.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In machine learning applications these functions $g_1,\\,g_2,\\,...,g_P$ are almost always of the same type - e.g., they can be convex quadratic functions with different constants parameterized by the same weights $\\mathbf{w}$.  \n",
    "\n",
    "\n",
    "- This special *summation structure* allows for a simple but very effective enhancement to virtually any local optimization scheme, and is called *mini-batch optimization*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A simple idea with powerful consequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Suppose we were to apply a local optimization scheme to minimize a function $g$ of the form\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(\\mathbf{w}\\right) = \\sum_{p=1}^P g_p\\left(\\mathbf{w}\\right).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where $g_1\\,g_2,\\,...,g_P$ are all functions of the same kind (e.g., quadratics with different constants parameterized by $\\mathbf{w}$).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- What would happen if we were to try to minimize $g$ by taking descent steps in the summand functions $g_1,\\,g_2,\\,...,g_P$ one-at-a-time?   \n",
    "\n",
    "\n",
    "- As we will see empircaly throughout this text, starting with the examples below, in many instances this idea can actually lead to considerably faster optimization of a function $g$ consisting of a sum of $P$ functions as detailed in general above above.\n",
    "\n",
    "\n",
    "- The gist of this idea is drawn graphically in the figure below for the case $P = 3$, where we compare the idea of taking a the a descent step simultaneously in $g_1,\\,g_2,\\,...,g_P$ versus a sequence of $P$ descent steps in $g_1$ then $g_2$ etc., up to $g_P$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src= '../../mlrefined_images/math_optimization_images/batch_vs_miinbatch_functions.png' width=\"80%\" height=\"auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Descending with larger mini-batch sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Instead of taking $P$ sequential steps in single functions $g_p$ (a mini-batch of *size $1$*) one-at-a-time, we can more general (with functions $g$ that take the form $g\\left(\\mathbf{w}\\right) = \\sum_{p=1}^P g_p\\left(\\mathbf{w}\\right)$) take fewer steps in one epoch, but take each step with respect to *several* of the functions $g_p$ e.g., two functions at-a-time, or three functions at-a-time, etc.,.  \n",
    "\n",
    "\n",
    "- With this slight twist on the idea detailed above we take fewer steps per epoch but take each with respect to larger non-overlapping subsets of the functions $g_1,\\,g_2,\\,...,g_P$, but still sweep through each function exactly once per epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Mini-batch optimization general performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Is the trade-off - taking more steps per epoch with a mini-batch approach as opposed a full descent step - worth the extra effort?  Typically *yes*.  \n",
    "\n",
    "\n",
    "- Often in practice when minimizing machine learning functions an epoch of mini-batch steps like those detailed above will drastically outperform an analogous full descent step - often referred to as a *full batch* or simply a *batch* epoch in the context of mini-batch optimization.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- A prototypical comparison of a cost function history employing a batch and corresponding epochs of mini-batch optimization applied to the same hypothetical function $g$ with the same initialization $\\mathbf{w}^0$ is shown in the figure below.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src= '../../mlrefined_images/math_optimization_images/minibatch_functions.png' width=\"90%\" height=\"auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Because we take far more steps with the mini-batch approach and because each $g_p$ takes the same form, each epoch of the mini-batch approach typically outperforms its full batch analog.  \n",
    "\n",
    "\n",
    "- Even when taking into account that far more descent steps are taken during an epoch of mini-batch optimization the method often greatly outperforms its full batch analog."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "141.067px",
    "width": "251.1px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
