{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A.4 Advanced Gradient Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Many advanced first order gradient based methods have been developed in the machine learning community that essentially combine the momentum and normalized gradient ideas in various interesting ways.  \n",
    "\n",
    "\n",
    "- In this Section we detail several of these popular methods including RMSprop, and Adam, highlighting their connection to the two fundamental solutions methods discussed in the prior two Sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- You can toggle the code on and off in this presentation via the button below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlrefined_libraries'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ec8bc8ddc925>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# import custom plotting tools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmlrefined_libraries\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath_optimization_library\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlrefined_libraries\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcalculus_library\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcallib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mstatic_plotter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_plotter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVisualizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlrefined_libraries'"
     ]
    }
   ],
   "source": [
    "## This code cell will not be shown in the HTML version of this notebook\n",
    "# import standard tools\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import autograd.numpy as np\n",
    "import time\n",
    "\n",
    "# import custom plotting tools\n",
    "from mlrefined_libraries import math_optimization_library as optlib\n",
    "from mlrefined_libraries import calculus_library as callib\n",
    "static_plotter = optlib.static_plotter.Visualizer();\n",
    "anime_plotter = optlib.animation_plotter.Visualizer();\n",
    "\n",
    "# The next three lines are needed to compensate for matplotlib notebook's tendancy to blow up images when plotted inline\n",
    "%matplotlib notebook\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combining momentum with normalized gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In [Section 3.7](https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_7_Problems.html) we described the notion of *momentum accelerated gradient descent*, and how it is a natural remedy for the *zig-zagging* problem the standard gradient descent algorithm suffers from when run along *long narrow valleys*.  \n",
    "\n",
    "\n",
    "- As we the momentum acceleration descent direction $\\mathbf{d}^{k-1}$ is simply an *exponential average* of gradient descent directions taking the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{d}^{k-1} = \\beta \\, \\mathbf{d}^{k-2}  - \\left(1 - \\beta\\right)\\nabla g\\left(\\mathbf{w}^{k-1}\\right) \\\\\n",
    "\\mathbf{w}^{\\,k} = \\mathbf{w}^{\\,k-1} + \\alpha \\, \\mathbf{d}^{k-1} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,     \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,      \\,\\,\\,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\beta \\in \\left[0,1 \\right]$ is typically set at a value of $\\beta = 0.8$ or higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Then in [Section 3.9](https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_9_Normalized.html) we saw how *normalizing the gradient descent direction componentwise* helps deal with the problem standard gradient descent has when traversering *flat regions* of a function.  \n",
    "\n",
    "\n",
    "- We saw there how a component-normalized gradient descent step takes the form (for the $j^{th}$ component of $\\mathbf{w}$)\n",
    "\n",
    "\\begin{equation}\n",
    "w_j^k = w_j^{k-1} - \\alpha \\, \\frac{\\frac{\\partial}{\\partial w_j}g\\left(\\mathbf{w}^{k-1}\\right)}{{\\sqrt{\\left(\\frac{\\partial}{\\partial w_j}g\\left(\\mathbf{w}\\right)\\right)^2}}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where in practice of course a small fixed value $\\epsilon > 0$ is often added to the denominator on the right hand side to avoid division by zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Knowing that these two additions to the standard gradient descent step help solve two fundamental problems associated with gradient descent, it is natural to then try to *combine them* to leverage both enhancements.  \n",
    "\n",
    "\n",
    "- There are several ways one might think to combine these two ideas.  For example, one could momentum acclerate a componentwise-normalized direction or - in other words - replace the gradient descent direction in the exponential average in equation (1) with its component-normalized version shown in equation (2). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Another way of combining the two ideas would be to *component-normalize the exponential average descent direction computed in momentum-accelerated gradient descent*.  That is, compute the exponential average direction in the top line of equation (1) and then normalize *it* (instead of the raw gradient descent direction) as shown in equation (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Doing this - and writing out the update for only the $j^{th}$ component of the resulting step - we have\n",
    "\n",
    "\\begin{equation}\n",
    "d^{k-1}_j = \\beta \\, d^{k-2}_j  - \\left(1 - \\beta\\right)\\frac{\\partial}{\\partial w_j}g\\left(\\mathbf{w}^{k-1}\\right) \\\\\n",
    "d^{k-1}_j \\longleftarrow  \\frac{d^{k-1}_j }{\\sqrt{\\left(d^{k-1}_j \\right)^2}}\\\\\n",
    "\\end{equation}\n",
    "\n",
    "where in practice of course a small $\\epsilon > 0$ (like e.g., $\\epsilon = 10^{-8}$) is added to the denominator to avoid division by zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With a full direction $\\mathbf{d}^{k-1}$ commputed in this way we can then take a descent step\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w}^{\\,k} = \\mathbf{w}^{\\,k-1} + \\alpha \\, \\mathbf{d}^{k-1}. \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many popular first order steps used to tune machine learning models employing deep neural networks combine momentum and normalized gradient descent in this sort of way.  Below we list a few examples including the popular *Adam* and *RMSprop* first order steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example. </span>  Adaptive Moment Estimation (Adam) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Adaptive Moment Estimation (Adam) is a componentwise-normalized gradient step employing independently calculated exponential averages for both the descent direction *and* magnitude.  That is, we compute $j^{th}$ coordinate of the updated descent direction by first computing the exponential average of the gradient descent direction $d_j^{k}$  squared magnitude $h_j^{k}$ separately along this coordinate as \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}\n",
    "\\\n",
    "d^{k-1}_j = \\beta_1 \\, d^{k-2}_j  + \\left(1 - \\beta_1\\right)\\frac{\\partial}{\\partial w_j}g\\left(\\mathbf{w}^{k-1}\\right) \\\\\n",
    "h_j^{k-1} = \\beta_2 \\, h_j^{k-2} + \\left(1 - \\beta_2\\right)\\left(\\frac{\\partial}{\\partial w_j}g\\left(\\mathbf{w}^{k-1}\\right)\\right)^2\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\beta_1$ and $\\beta_2$ lie in the range $[0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Popular values the parameters of this update step are $\\beta_1 = 0.9$, $\\beta_2 = 0.999$.\n",
    "\n",
    "Note as with any exponential average these two updates apply when $k-1  > 0$ and should be initialized at first values from the series they respectively model: that is the initial descent direction $d^0_j = \\frac{\\partial}{\\partial w_j}g\\left(\\mathbf{w}^{0}\\right)$ and its squared magnitude $h^0_j = \\left(\\frac{\\partial}{\\partial w_j}g\\left(\\mathbf{w}^{0}\\right)\\right)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *Adam* step is then a component-normalized descent step using this exponentially average descent direction and magnitude.  A step in the $j^{th}$ coordinate then takes the form\n",
    "\n",
    "\\begin{equation}\n",
    "w_j^k = w_j^{k-1} - \\alpha \\frac{d^{k-1}_j}{\\sqrt{h_j^{k-1}}}.\n",
    "\\end{equation}\n",
    "\n",
    "where in practice of course a small $\\epsilon > 0$ (like e.g., $\\epsilon = 10^{-8}$) is added to the denominator to avoid division by zero.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice - as we saw the (component) normalized step in the previous Section - that if we slightly re-write above as\n",
    "\n",
    "\\begin{equation}\n",
    "w_j^k = w_j^{k-1} - \\frac{\\alpha}{\\sqrt{h_j^{k-1}}} \\, d^{k-1}_j.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "we can interpret the Adam step as a momentum-accelerated gradient descent step with an individual steplength / learning rate value $\\frac{\\alpha}{\\sqrt{h_j^{k-1}}}$ per component that all *adjusts themselves individually at each step based on component-wise exponentially normalized magnitude of the gradient*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example.</span> Root Mean Squared Propogation (RMSprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This popular first order step is a varient of the *component-normalized* step discussed in [Section 3.9](https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_9_Normalized.html), where each component of the gradient is normalized by an *exponential average* of the magnitude of previously computed gradient descent directions.  \n",
    "\n",
    "In other words, we compute an exponential average of the *magnitude* of the gradient at each step.  Denoting $h_j^{k}$ the exponential average of of the squared magnitude of the $j^{th}$ partial derivative at step $k$ we have\n",
    "\n",
    "\\begin{equation}\n",
    "h_j^k = \\gamma \\, h_j^{k-1} + \\left(1 - \\gamma\\right)\\left(\\frac{\\partial}{\\partial w_j}g\\left(\\mathbf{w}^{k-1}\\right)\\right)^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Root Mean Squared Error Propogation (RMSprop) step is then a component-normalized descent step using this *exponential average*.  A step in the $j^{th}$ coordinate then takes the form\n",
    "\n",
    "\\begin{equation}\n",
    "w_j^k = w_j^{k-1} - \\alpha \\frac{\\frac{\\partial}{\\partial w_j} g\\left(\\mathbf{w}^{k-1}\\right)}{\\sqrt{h_j^{k-1}}}\n",
    "\\end{equation}\n",
    "\n",
    "where in practice of course a small $\\epsilon > 0$ (like e.g., $\\epsilon = 10^{-8}$) is added to the denominator to avoid division by zero.  Popular values the parameters of this update step are $\\beta = 0.9$ and $\\alpha = 10^{-2}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice - as we saw the (component) normalized step in the previous Section - that if we slightly re-write above as\n",
    "\n",
    "\\begin{equation}\n",
    "w_j^k = w_j^{k-1} - \\frac{\\alpha}{\\sqrt{h_j^{k-1}}} \\, \\frac{\\partial}{\\partial w_j}g\\left(\\mathbf{w}^{k-1}\\right).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "we can interpret the RMSprop step as a standard gradient descent step with an individual steplength / learning rate value $\\frac{\\alpha}{\\sqrt{h_j^{k-1}}}$ per component that all *adjusts themselves individually at each step based on component-wise magnitude of the gradient*."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "103px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
