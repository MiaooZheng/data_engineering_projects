{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Chapter 12: Kernel methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 12.5  Optimization of Kernelized Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Because the final kernelized model remains linear in its parameters (the kernels themselves having no internal parameters tuned during optimization), corresponding kernelized cost functions themselves are quite 'nice' in terms of their general shape. \n",
    "\n",
    "- For example, any convex cost function for regression and classification *remains convex when kernelized*.\n",
    "\n",
    "- This allows virtually any optimization method to be used to tune a kernelized supervised learner - from zero to first order and even powerful second order approaches like Newton's method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Notice, because kernel matrices $\\mathbf{H}$ are sized $P \\times P$ - where $P$ is the size of the training set - they inherently scale very poorly in the size of training data.\n",
    "\n",
    "- For example, with $P=10,000$ the corresponding kernel matrix will be of size $10,000 \\times 10,000$, with $10^8$ values to store, far more than a modern computer can store all at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Most standard ways of dealing with this crippling scaling issue revolve around avoiding the creation of the entire kernel matrix $\\mathbf{H}$, especially during training.\n",
    "\n",
    "- For example, one can use first order methods such as stochastic gradient descent to avoid construction of the entire kernel $\\mathbf{H}$ during training. \n",
    "\n",
    "- Sometimes the explicit structure of certain problems allows can allow for the avoidance of explicit kernel construction as well."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "49px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
