{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 9.4  Imputing missing values in a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Real world data can contain *missing values* due to human error in collection, storage issues, faulty sensors, etc.,  \n",
    "\n",
    "\n",
    "- If a supervised learning datapoint is missing its *output* value - e.g., if a classification datapoint is missing its *label* - there can be little we can do to salvage the datapoint, and usually such a corrupted datapoint is thrown away.  \n",
    "\n",
    "\n",
    "- Likewise, if a large number of the values of an input are missing the data is best discarded.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- However a datapoint whose input is missing just a handful of *input (feature) values* can be salvaged, and such points should be salvaged since data is often a scarce resource.  \n",
    "\n",
    "\n",
    "- For us to be able to use a datapoint with missing entries in training any machine learning model we must *fill in missing input features with numerical values* \n",
    "\n",
    "\n",
    "- This is often called *imputing*.  \n",
    "\n",
    "\n",
    "- But what values should we set missing entries too? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imputing using the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Suppose we have a set of $P$ inputs, each of which is $N$ dimensional, and say that the $j^{th}$ point has is missing its $n^{th}$ input feature value.  \n",
    "\n",
    "\n",
    "- In other words, the value $x_{j,n}$ is missing from our input data.  \n",
    "\n",
    "\n",
    "- A reasonable value to fill for this missing entry is simply the *average of the dataset along this dimension*.\n",
    "\n",
    "\n",
    "- This can roughly be considered the 'standard' value of our input along this dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- If in particular we use the *mean* $\\mu_n$ of our input along this dimension (ignoring our missing entry $x_{j,n}$) we set $x_{j,n} = \\mu_n$ where\n",
    " \n",
    "\\begin{equation}\n",
    "\\mu_n = \\frac{1}{P-1} \\sum_{\\underset{p \\neq j}{j=1}}^P x_{p,n}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- This is often called *mean-imputation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Notice how, in mean-imputing $x_{p,n}$, that the mean value of the entire $n^{th}$ feature of the input with the inputed value *does not change*.  \n",
    "\n",
    "\n",
    "- We can see this by simply computing the mean of the new $n^{th}$ input dimension as\n",
    "\n",
    "$$\n",
    "\\frac{1}{P} \\sum_{p=1}^P x_{p,n} = \\frac{1}{P} \\sum_{\\underset{p \\neq j}{j=1}}^P  x_{p,n} + \\frac{1}{P} x_{j,n} \\\\= \\frac{P - 1}{P} \\frac{1}{P-1} \\sum_{\\underset{p \\neq j}{j=1}}^Px_{p,n} + \\frac{1}{P} x_{j,n} = \\frac{P - 1}{P} \\mu_n + \\frac{1}{P} \\mu_n = \\mu_n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Also notice that if we impute missing values of a dataset *using the mean along each input dimension* that if we then *standard normalize* this dataset, *all values imputed with the mean become exactly zero*.  \n",
    "\n",
    "\n",
    "- Because the mean of an input feature that has been imputed with mean values does not change, when we *mean-center* the dataset (the first step in standard normalization) by *subtracting* the mean of this input dimension the resulting value becomes exactly zero (this is illustrated in the figure below for a simple case where $N = 2$).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "- Any model weight touching such a mean-imputed entry is then comopletely nullified numerically.  \n",
    "\n",
    "\n",
    "- In other words, in training any mean-imputed value will not directly contribute to the tuning of its associated feature weight (provided the input has been standard normalized).  \n",
    "\n",
    "\n",
    "\n",
    "- This is very desireable given that such values were missing to begin with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- More generally if our input is missing multiple entries along this dimension we can just as reasonably replace each one with the mean along the $n^{th}$ feature, where in computing this mean we once again exclude missing values.  \n",
    "\n",
    "\n",
    "- Again one can see that with these imputed values the *mean along this dimension does not change*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<figure>\n",
    "      <img src= '../../mlrefined_images/superlearn_images/mean_inpute_stadnard_normal.png' width=\"70%\"  height=\"auto\" alt=\"\"/>\n",
    "  <figcaption>  <em> \n",
    "(left panel) The input of a prototypical $N = 2$ dimensional dataset where a single point $\\mathbf{x}_j$,  drawn as a *hollow* red dot, is missing its second entry.  The mean-imputed version of this point is then shown as a *filled-in* red point.  (right panel) By standard-normalizing such a dataset the mean-imputed value of $\\mathbf{x}_j$ becomes *exactly equal to zero*.\n",
    "</em>  </figcaption> \n",
    "</figure>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "68px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
