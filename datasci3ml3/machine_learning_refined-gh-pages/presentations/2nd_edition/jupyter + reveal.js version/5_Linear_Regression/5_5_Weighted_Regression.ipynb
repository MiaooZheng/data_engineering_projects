{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.5 Weighted Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Because regression cost functions are *summable over individual points* we can - as we will see in this Section - weight individual points in order to emphasize or de-emphasize their importance to a regression model.  \n",
    "\n",
    "\n",
    "- This is called *weighted regression*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- You can toggle the code on and off in this presentation via the button below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## This code cell will not be shown in the HTML version of this notebook\n",
    "# imports from custom library\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from mlrefined_libraries import superlearn_library as superlearn\n",
    "from mlrefined_libraries import math_optimization_library as optlib\n",
    "\n",
    "# demos for this notebook\n",
    "regress_plotter = superlearn.lin_regression_demos\n",
    "optimizers = optlib.optimizers\n",
    "static_plotter = optlib.static_plotter.Visualizer()\n",
    "datapath = '../../mlrefined_datasets/superlearn_datasets/'\n",
    "\n",
    "# import autograd functionality to bulid function's properly for optimizers\n",
    "import autograd.numpy as np\n",
    "\n",
    "# import timer\n",
    "from datetime import datetime \n",
    "\n",
    "# This is needed to compensate for %matplotlib notebook's tendancy to blow up images when plotted inline\n",
    "%matplotlib notebook\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dealing with duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Imagine we have a linear regression dataset $\\left(x_1,y_1\\right),\\left(x_2,y_2\\right),...,\\left(x_P,y_P\\right)$ that contains multiple copies of the same point, which can occur in a variety of contexts including:\n",
    "\n",
    "    \n",
    "- experimental data (e.g., in physics, medicine, etc.): if a repeated experiment produces the same result.\n",
    "    \n",
    "    \n",
    "- metadata-type datasets (e.g., census, customer databases): due to necessary / helpful pre-processing that quantizes (bins) input features in order to make human-in-the-loop analysis of the data / modeling easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In such instances 'duplicate' datapoints should not be thrown away, since they accurately represent the true phenomenon under study.\n",
    "\n",
    "\n",
    "- If we examine any regression cost function over such a dataset (i.e., one with repeated entries) we can see that it naturally *collapses* into a weighted version itself.  \n",
    "\n",
    "\n",
    "- For example, let us examine the Least Squares cost and suppose that our first two datapoints  $\\left(x_1,\\,y_1\\right)$ and $\\left(x_2,\\,y_2\\right)$ are *identical*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In this instance -- using our `model` notation -- the first two summands of our cost function (in the first two datapoints) can be combined since they will always take on the same value\n",
    "\n",
    "$$\\frac{1}{P}\\sum_{p=1}^P \\left(\\text{model}\\left(x_p,\\mathbf{w}\\right) - y_p\\right)^2 \\\\\n",
    "= \\frac{1}{P}\\,2\\,\\left(\\text{model}\\left(x_1,\\mathbf{w}\\right) - y_1\\right)^2 + \\frac{1}{P}\\,0\\,\\left(\\text{model}\\left(x_2,\\mathbf{w}\\right) - y_2\\right)^2 \\\\ + \\frac{1}{P}\\,\\sum_{p=3}^P \\left(\\text{model}\\left(x_p,\\mathbf{w}\\right) - y_p\\right)^2 \\\\\n",
    "= \\frac{1}{P}\\,2\\,\\left(\\text{model}\\left(x_1,\\mathbf{w}\\right) - y_1\\right)^2 + \\frac{1}{P}\\,\\sum_{p=3}^P \\left(\\text{model}\\left(x_p,\\mathbf{w}\\right) - y_p\\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Here we can see that the cost function naturally collapses so that a repeated point in a dataset is represented *in the cost function* by a single weighted summand.  \n",
    "\n",
    "\n",
    "- Of course this holds more generally as well.  \n",
    "\n",
    "\n",
    "- If we examined a regression cost function of a dataset having any number of identical points then we can collapse the summands of this cost for each set of identical points just as we have seen here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In general this leads to the notion that each term in a regression cost can be *weighted* to reflect repeated points.  \n",
    "\n",
    "\n",
    "- We can write such a *weighted regression* Least Squares as\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(\\mathbf{w}\\right) = \\frac{1}{P}\\sum_{p=1}^P \\beta_p \\left(\\text{model}\\left(x_p,\\mathbf{w}\\right) - y_p\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- Here $\\beta_1,\\,\\beta_2,\\,...,\\,\\beta_P$ are *point-wise* weights. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- That is, a unique point $\\left(x_p,\\,y_p\\right)$ in the dataset has weight $\\beta_p = 1$ whereas if this point is repeated $R$ times in the dataset then one instance of it will have weight $\\beta_p = R$ while the others have weight $\\beta_p = 0$.\n",
    "\n",
    "\n",
    "- Since these weights are fixed (i.e., they are *not* parameters that need to be tuned, like $\\mathbf{w}$) we can minimize a weighted regression cost precisely as we would any other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## This code cell will not be shown in the HTML version of this notebook\n",
    "# a Python implementation of the weighted least squares cost function\n",
    "# setup to compute over mini-batches if desired\n",
    "def least_squares(w,x,y,beta,iter):\n",
    "    # get batch of points\n",
    "    x_p = x[:,iter]\n",
    "    y_p = y[:,iter]\n",
    "    beta_p = beta[:,iter]\n",
    "\n",
    "    # compute cost over batch\n",
    "    cost = np.sum((beta*model(x_p,w) - y_p)**2)\n",
    "    return cost/float(np.size(y_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Weighting points by confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Weighted regression can also be employed when - given knowledge of the process generating a dataset - we want to weight each point based on our *confidence on the trustworthiness* of each datapoint.  \n",
    "\n",
    "\n",
    "- For example if our dataset came in two batches - one batch from a trustworthy source and another from a less trustworthy source (where some datapoints could be noisy or fallacious) - we would want to weight datapoints from the trustworthy source more in our final regression.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "- We can do this very easily using precisely the weighted regression paradigm introduced above, only now we *set the weights $\\beta_1,\\,\\beta_2,\\,...,\\,\\beta_P$ ourselves based on our confidence of each point*.  \n",
    "\n",
    "\n",
    "- If we believe that a point is very trustworthy we can set its corresponding weight $\\beta_p$ closer to $1$, and the more untrustworthy we find a point the smaller we set $\\beta_p$ in the range $0 \\leq \\beta_p \\leq 1$ where $\\beta_p = 0$ implies we do not trust the point at all.  \n",
    "\n",
    "\n",
    "- In making these weight selections we of course determine *how important each datapoint is* in the training of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Below we show the result of increasing the confidence / weight $\\beta_p$ on a *single point* in a toy dataset.\n",
    "\n",
    "\n",
    "- and how this effects a fully trained regression model on a toy linear regression dataset.  \n",
    "\n",
    "\n",
    "- This single point is colored red and we show its diameter increasing and as we increase its corresponding weight $\\beta_p$.  \n",
    "\n",
    "\n",
    "- With each weighting a weighted Least Squares cost is completely minimized over the entire dataset, and resulting line fit to data (and shown in black). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## This code cell will not be shown in the HTML version of this notebook\n",
    "# show demo\n",
    "savepath = 'videos/animation_4.mp4'\n",
    "csvname = datapath + 'weighting_regression_data.csv'\n",
    "demo2 = superlearn.weighted_regression_animator.Visualizer()\n",
    "demo2.animate_weighting(savepath,csvname,num_slides = 200,special_ind=9,fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"1000\" height=\"400\" controls loop>\n",
       "  <source src=\"videos/animation_4.mp4\" type=\"video/mp4\">\n",
       "  </video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This code cell will not be shown in the HTML version of this notebook\n",
    "# load video into notebook\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"1000\" height=\"400\" controls loop>\n",
    "  <source src=\"videos/animation_4.mp4\" type=\"video/mp4\">\n",
    "  </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " - The higher the weighting of this single point, the more we incentivize a linear regressor to fit to it.   \n",
    " \n",
    " \n",
    " - If we increase its weight enough the fully trained regression model naturally starts fitting to this single datapoint alone (disregarding all other points)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "84px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
