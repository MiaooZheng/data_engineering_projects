{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2.1  Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The problem of determining the smallest (or largest) value a function can take, referred to as its *global minimum* (or *global maximum*) is a centuries old pursuit that has numerous applications throughout the sciences and engineering.  \n",
    "\n",
    "\n",
    "- In this Chapter we begin our investigation of mathematical optimization by describing the *zero order optimization* techniques (also called *Hessian free optimization*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- While not always the most powerful optimization tools at our disposal, these techniques are quite simple and often quite effective.\n",
    "\n",
    "- Discussing zero order methods first also allows us to lay bear, in a simple setting, a range of crucial concepts we will see throughout the Chapters that follow in more complex settings.\n",
    "\n",
    "- These concepts include the notions of *optimality*, *local optimization*, *descent directions*, *steplengths*, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing minima and maxima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- When a function takes in only one or two inputs we can visually identify its minima or maxima by plotting it over a large swath of its input space.\n",
    "\n",
    "- But what if a function takes in more than two inputs?  \n",
    "\n",
    "- We begin our discussion by first examining a number of low dimensional examples to gain an intuitive feel for how we might effectively identify these desired minima or maxima in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:#a50e3e;\">Example. </span> Visual inspection of simple functions for minima and maxima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Every machine learning problem has parameters that must be tuned properly to ensure optimal learning\n",
    "\n",
    "- For example, there are two parameters that must be properly tuned in the case of a simple linear regression.\n",
    "\n",
    "- That is, when fitting a line to a scatter of data: the slope and intercept of the linear model.\n",
    "\n",
    "- These two parameters are tuned by forming what is called a *cost function* or *loss function*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- This is a continuous function in both parameters that measures how well the linear model fits a dataset given a value for its slope and intercept. \n",
    "\n",
    "- The proper tuning of these parameters via the cost function corresponds geometrically to finding the values for the parameters that make the cost function *as small as possible*\n",
    "\n",
    "- Or, in other words, the parameters that *minimize* the cost function. \n",
    "\n",
    "- The image below illustrates how choosing a set of parameters higher on the cost function results in a corresponding line fit that is poorer than the one corresponding to parameters at the lowest point on the cost surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../../mlrefined_images/math_optimization_images/bigpicture_regression_optimization.png\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- This same idea holds true for regression with higher dimensional input, as well as classification where we must properly tune parameters to *separate* classes of data.\n",
    "\n",
    "- Again, the parameters minimizing an associated cost function provide the best classification result. This is illustrated for classification below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../../mlrefined_images/math_optimization_images/bigpicture_classification_optimization.png\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The tuning of these parameters require the *minimization of a cost function* can be formally written as follows.  \n",
    "\n",
    "\n",
    "- For a generic function $g(\\mathbf{w})$ taking in a general $N$ dimensional input $\\mathbf{w}$ the problem of finding the particular point $\\mathbf{v}$ where $g$ attains its smallest value is written formally as\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{\\mathbf{w}}{\\mbox{minimize}}\\,\\,\\,\\,g\\left(\\mathbf{w}\\right)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- This formal problem can very rarely be solved 'by hand', instead we must rely on algorithmic techniques for finding function minima (or at the very least finding points close to them).  \n",
    "\n",
    "\n",
    "- In this part of the text we examine many algorithmic methods of *mathematical optimization*, which aim to do just this."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "103px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
