{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# 2.4  Local optimization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- *Local optimization methods* work by taking a single sample input and refine it sequentially, driving it towards an approximate minimum point.  \n",
    "\n",
    "\n",
    "- Local optimization methods are by far the most popular mathematical optimization schemes used in machine learning today.\n",
    "\n",
    "\n",
    "- Indeed local optimization algorithms being the subject of the remaining of this as well as the next two Chapters. \n",
    "\n",
    "\n",
    "- While there is substantial variation in the kinds of specific local optimization methods we will discuss going forward, they all share a common overarching framework which we introduce in this Section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- You can toggle the code on and off in this presentation via the button below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# This code cell will not be shown in the HTML version of this notebook\n",
    "# imports from custom library\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import matplotlib.pyplot as plt\n",
    "from mlrefined_libraries import basics_library as baslib\n",
    "from mlrefined_libraries import math_optimization_library as optlib\n",
    "\n",
    "# this is needed to compensate for matplotlib notebook's tendancy to blow up images when plotted inline\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The big picture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- As we saw in the previous Section global optimization methods test a multitude of simultaneously sampled input points to determine an approximate minimum of a given function $g\\left(\\mathbf{w}\\right)$. \n",
    "\n",
    "\n",
    "- *Local optimization methods*, on the other hand, work in the opposite manner *sequentially* refining a single sample input called an *initial point* until it reaches an approximate minimum.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "- Starting with a sample input / initial point $\\mathbf{w}^0$, local optimization methods refine this initialization sequentially *pulling the point* 'downhill' towards points that are lower and lower on the function.\n",
    "\n",
    "\n",
    "- From $\\mathbf{w}^0$ the point is 'pulled' downhill to a new point $\\mathbf{w}^1$ lower on the function i.e., where $g\\left(\\mathbf{w}^0\\right) > g\\left(\\mathbf{w}^1 \\right)$.  The point $\\mathbf{w}^1$ then itself 'pulled' downwards to a new point $\\mathbf{w}^2$, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src= '../../mlrefined_images/math_optimization_images/Fig_2_6.png' width=\"75%\" height=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- This refining process yields a sequence of $K$ points (starting with our initializer) \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w}^0,\\,\\mathbf{w}^1,\\,...,\\mathbf{w}^K\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- Here each subsequent point is (again generally speaking) on a lower and lower portion of the function i.e.,\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(\\mathbf{w}^0\\right) > g\\left(\\mathbf{w}^1\\right)\\, > \\,\\cdots\\,> g\\left(\\mathbf{w}^K\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Unlike the global approach, a wide array of specific local optimization methods scale gracefully with input dimension.\n",
    "\n",
    "\n",
    "- This is because the sequential refinement process - which can be designed in a variety of ways - can be made extremely effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The general framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Every local method, regardless of form, works generally as follows.\n",
    "\n",
    "\n",
    "- To take the first step from an initial point $\\mathbf{w}^0$ to the very first update $\\mathbf{w}^1$ (which should be lower on the function than the initializer) what is called a *descent direction* is at $\\mathbf{w}^0$ is found.  \n",
    "\n",
    "\n",
    "- This is a direction vector $\\mathbf{d}^0$ in the input space that stems from $\\mathbf{w}^0$ - i.e., it begins at $\\mathbf{w}^0$ and points away from the initial point - that if followed leads to an input point with lower function evaluation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "- When such a direction is found the first update $\\mathbf{w}^1$ is then literally the sum\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w}^1 = \\mathbf{w}^0 + \\mathbf{d}^0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src= '../../mlrefined_images/math_optimization_images/local_method.png' width=\"200%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- To refine the point $\\mathbf{w}^1$ we look for a new descent direction $\\mathbf{d}^1$ - one that moves 'downhill' stemming from the point $\\mathbf{w}^1$.  \n",
    "\n",
    "\n",
    "- When we find such a direction the second update $\\mathbf{w}^2$ is then formed as the sum\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w}^2 = \\mathbf{w}^1 + \\mathbf{d}^1.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- And we keep going like this, determining further descent directions producing - in the end - a sequence of input points (starting with our initializer) that goes as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\\begin{equation}\n",
    "\\begin{array}\n",
    "\\\n",
    "\\mathbf{w}^0 \\\\\n",
    "\\mathbf{w}^1 = \\mathbf{w}^0 + \\mathbf{d}^0  \\\\\n",
    "\\mathbf{w}^2 = \\mathbf{w}^1 + \\mathbf{d}^1  \\\\\n",
    "\\,\\,\\vdots \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, \\vdots \\,\\,\\,\\,\\,\\,\\,\\, \\vdots\\\\\n",
    "\\mathbf{w}^K = \\mathbf{w}^{K-1} + \\mathbf{d}^{K-1}\n",
    "\\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- Here $\\mathbf{d}^{k-1}$ is the descent direction defined at the $k^{th}$ step of the process: $\\begin{equation}\n",
    "\\mathbf{w}^{k} = \\mathbf{w}^{k-1} + \\mathbf{d}^{k-1} \n",
    "\\end{equation}$\n",
    "\n",
    "\n",
    "- And we have $g(\\mathbf{w}^0) > g(\\mathbf{w}^1) > g(\\mathbf{w}^2) > \\cdots > g(\\mathbf{w}^{K}) $.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- How are these *descent directions* stemming - stemming from each subsequent update - actually found?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- There are a multitude of ways of determining proper descent directions - indeed in the remaining Sections of this Chapter we discuss *zero-order* (also called *Hessian free*) approaches for doing this.\n",
    "\n",
    "\n",
    "- In the following Chapters we describe *first* and *second* order approaches (i.e., approaches that leverage the first and/or second derivative of a function to determine descent directions).  \n",
    "\n",
    "\n",
    "- In other words - it is precisely this - how the descent directions are determined - which distinguishes local optimization schemes from one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The steplength parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We can easily calculate precisely how far we travel at the $k^{th}$ step of a generic local optimization scheme.  \n",
    "\n",
    "\n",
    "- Measuring the distance traveled from the previous $\\left(k-1\\right)^{th}$ point we can see that we move a distance precisely equal to the length of the $\\left(k-1\\right)^{th}$ descent direction as\n",
    "\n",
    "\\begin{equation}\n",
    "\\left \\Vert\\mathbf{w}^{k} - \\mathbf{w}^{k-1}  \\right\\Vert_2 = \\left \\Vert\\ \\left(\\mathbf{w}^{k-1} + \\mathbf{d}^{k-1}\\right) - \\mathbf{w}^{k-1}  \\right\\Vert_2 =  \\left \\Vert  \\mathbf{d}^{k-1} \\right \\Vert_2.\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Depending on how our descent directions are generated we may or may not have control over their length (all we ask is that they point in the right direction, 'down hill').  \n",
    "\n",
    "\n",
    "- This can mean even if they point in the right direction - towards input points that are lower on the function - that their *length* could be problematic.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- If for example, they are too long then a local method can oscillate wildly at each update step never reaching an approximate minimum, or likewise if they are too short *in length* a local method we move so sluggishly slow that far too many steps would be required to reach an approximate minimum.\n",
    "\n",
    "\n",
    "- Both of these potentialities are illustrated in the Figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src= '../../mlrefined_images/math_optimization_images/steplength.png' width=\"100%\" height=\"auto\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Because of this potential problem many local optimization schemes come equipped with what is called a *steplength parameter* (also called a *learning rate* parameter).\n",
    "\n",
    "\n",
    "- We control this value, and it helps us more directly control the length of each update step (hence the name *steplength parameter*).\n",
    "\n",
    "\n",
    "- This is simply a parameter - typically denoted by the Greek letter $\\alpha$ - that is added to the generic local optimization update scheme described above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "- With a steplength parameter the generic $k^{th}$ update step in equation (6) is written analogously as \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w}^{k} = \\mathbf{w}^{k-1} + \\alpha \\mathbf{d}^{k-1}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The only difference between this form for the $k^{th}$ step and the original is that now we scale the descent direction $\\mathbf{d}^{k-1}$ by the parameter $\\alpha$ - which we can set as we please.  \n",
    "\n",
    "\n",
    "- Re-calculating the distance traveled by the $k^{th}$ step with this added parameter we can see that\n",
    "\n",
    "\\begin{equation}\n",
    "\\left \\Vert\\mathbf{w}^{k} - \\mathbf{w}^{k-1}  \\right\\Vert_2 = \\left \\Vert\\ \\left(\\mathbf{w}^{k-1} + \\alpha \\mathbf{d}^{k-1}\\right) - \\mathbf{w}^{k-1}  \\right\\Vert_2 = \\alpha  \\left \\Vert  \\mathbf{d}^{k-1} \\right \\Vert_2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In other words the length of the $k^{th}$ step is now proportional to the descent direction, and we can fine tune precisely how far we wish to travel in its direction by setting $\\alpha$ as we please.\n",
    "\n",
    "\n",
    "- A common choice is to set $\\alpha$ to some fixed small constant value for each of the $K$ steps - but like local optimization methods themselves there are a number of ways of usefully setting the steplength parameter which we will discuss in the future."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "103px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
